[["index.html", "Reticular Action Model (RAM) Notation Notes Chapter 1 Description", " Reticular Action Model (RAM) Notation Notes Ivan Jacob Agaloos Pesigan 2021-01-19 Chapter 1 Description This is a collection of my personal notes on the Reticular Action Model (RAM) notation that accompanies the ramR package (Pesigan 2021). You can install the released version of ramR from GitHub with: remotes::install_github(&quot;jeksterslab/ramR&quot;) These notes are based on the following resources: Boker and McArdle (2005) McArdle and McDonald (1984) McArdle (2005) See GitHub Pages for the html deployment. "],["ram-matrix-notation.html", "Chapter 2 Reticular Action Model (RAM) Matrix Notation 2.1 Full Model 2.2 Observed/Manifest/Given Variables vs. Unobserved/Latent/Hidden Variables", " Chapter 2 Reticular Action Model (RAM) Matrix Notation 2.1 Full Model Definition 2.1 \\[\\begin{equation} \\mathbf{v} = \\mathbf{A} \\mathbf{v} + \\mathbf{u} \\end{equation}\\] where \\(\\mathbf{v}\\) and \\(\\mathbf{u}\\) are \\(t \\times 1\\) vectors of random variables \\(\\mathbf{A}\\) is a \\(t \\times t\\) matrix of directed or asymmetric relationship from column variable \\(v_j\\) to row variable \\(v_i\\) \\(\\mathbf{A}\\) represent the regression of each of the \\(t\\) variables \\(\\mathbf{v}\\) on the other \\(t - 1\\) variables diagonal \\(a_{i,i}\\) is zero \\(u_i\\) represent the residual of \\(v_i\\) if all regression coefficients on other variables are zero, then the variable \\(v_i\\) is considered the same as its own residual \\(u_i\\) Definition 2.2 \\[\\begin{equation} \\mathbf{S} = \\mathbb{E} \\left\\{ \\mathbf{u} \\mathbf{u}^{\\prime} \\right\\} , \\end{equation}\\] where \\(\\mathbf{S}\\) is a \\(t \\times t\\) matrix of undirected or symmetric relationship the notation \\(\\boldsymbol{\\Omega}\\) is used in other sources for \\(\\mathbf{S}\\) \\(\\mathbb{E}\\) is the expectation operator Definition 2.3 \\[\\begin{equation} \\mathbf{C} = \\mathbb{E} \\left\\{ \\mathbf{v} \\mathbf{v}^{\\prime} \\right\\} , \\end{equation}\\] where \\(\\mathbf{C}\\) is a \\(t \\times t\\) variance-covariance matrix the notation \\(\\boldsymbol{\\Sigma}\\) is used in other sources for \\(\\mathbf{C}\\) Definition 2.4 \\[\\begin{equation*} \\mathbf{v} = \\mathbf{A} \\mathbf{v} + \\mathbf{u} \\end{equation*}\\] can be rewritten as \\[\\begin{equation} \\begin{split} \\mathbf{v} - \\mathbf{A} \\mathbf{v} &amp;= \\mathbf{u} \\\\ \\mathbf{u} &amp;= \\mathbf{v} - \\mathbf{A} \\mathbf{v} \\\\ \\mathbf{u} &amp;= \\left( \\mathbf{I} - \\mathbf{A} \\right) \\mathbf{v} \\end{split} \\end{equation}\\] assuming that \\(\\left( \\mathbf{I} - \\mathbf{A} \\right)\\) is non-singular, \\[\\begin{equation} \\mathbf{E} = \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\end{equation}\\] then \\[\\begin{equation} \\begin{split} \\mathbf{v} &amp;= \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\mathbf{u} \\\\ &amp;= \\mathbf{E} \\mathbf{u} . \\end{split} \\end{equation}\\] Using the definitions above, \\(\\mathbf{S}\\) and \\(\\mathbf{C}\\) are given by \\[\\begin{equation} \\begin{split} \\mathbf{S} &amp;= \\left( \\mathbf{I} - \\mathbf{A} \\right) \\mathbf{C} \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\\\ &amp;= \\mathbf{E}^{-1} \\mathbf{C} \\left( \\mathbf{E}^{-1} \\right)^{\\mathsf{T}} \\end{split} \\end{equation}\\] \\[\\begin{equation} \\begin{split} \\mathbf{C} &amp;= \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\mathbf{S} \\left[ \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\right]^{\\mathsf{T}} \\\\ &amp;= \\mathbf{E} \\mathbf{S} \\mathbf{E}^{\\mathsf{T}} \\end{split} \\end{equation}\\] 2.2 Observed/Manifest/Given Variables vs. Unobserved/Latent/Hidden Variables Definition 2.5 \\[\\begin{equation} \\mathbf{v} = \\begin{bmatrix} \\mathbf{g}_{p \\times 1} \\\\ \\mathbf{h}_{q \\times 1} \\\\ \\end{bmatrix} \\end{equation}\\] \\[\\begin{equation} t = p + q \\end{equation}\\] \\(\\mathbf{g}\\) may be considered observed, manifest or given variables \\(\\mathbf{h}\\) may be considered unobserved, latent, or hidden variables Definition 2.6 \\[\\begin{equation} \\mathbf{F} = \\begin{bmatrix} \\mathbf{I}_{p \\times p} : \\mathbf{0}_{p \\times q} \\end{bmatrix} \\end{equation}\\] the \\(\\mathbf{F}\\) matrix acts as a filter to select the manifest variables out of the full set of manifest and latent variables \\[\\begin{equation} \\mathbf{g} = \\mathbf{F} \\mathbf{v} \\end{equation}\\] \\[\\begin{equation} \\begin{split} \\mathbf{g} &amp;= \\mathbf{F} \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\mathbf{u} \\\\ &amp;= \\mathbf{F} \\mathbf{E} \\mathbf{u} \\end{split} \\end{equation}\\] Definition 2.7 \\[\\begin{equation} \\mathbf{M} = \\mathbb{E} \\left\\{ \\mathbf{g} \\mathbf{g}^{\\mathsf{T}} \\right\\} \\end{equation}\\] \\[\\begin{equation} \\begin{split} \\mathbf{M} &amp;= \\mathbf{F} \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\mathbf{S} \\left[ \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\right]^{\\mathsf{T}} \\mathbf{F}^{\\mathsf{T}} \\\\ &amp;= \\mathbf{F} \\mathbf{E} \\mathbf{S} \\mathbf{E}^{\\mathsf{T}} \\mathbf{F}^{\\mathsf{T}} \\\\ &amp;= \\mathbf{F} \\mathbf{C} \\mathbf{F}^{\\mathsf{T}} \\end{split} \\end{equation}\\] when components of \\(\\mathbf{v}\\) are permuted, the columns of \\(\\mathbf{F}\\) can be correspondingly permuted the rows and columns of \\(\\mathbf{C}\\) that are filtered out by \\(\\mathbf{F}\\) contain useful information about the latent variable structure. The equations above completely define RAM. "],["ram-diagram.html", "Chapter 3 Reticular Action Model (RAM) Path Diagram", " Chapter 3 Reticular Action Model (RAM) Path Diagram Figure 3.1: Path Diagram Elements Figure 3.2: Two-Variable Regression Model Figure 3.3: \\(k\\)-Variable Regression Model Figure 3.4: Two-Factor Confirmatory Factor Analysis Model "],["ram-t.html", "Chapter 4 Student’s \\(t\\)-test 4.1 Symbolic 4.2 Numerical Example", " Chapter 4 Student’s \\(t\\)-test In this section, the Student’s \\(t\\)-test is presented as a structural equation model using the RAM notation. Let \\(y\\) be a continuous dependent variable, \\(x\\) be a dichotomous independent variable \\(\\left( x = \\{0, 1\\} \\right)\\), and \\(\\varepsilon\\) be the stochastic error term with mean 0 and constant variance of \\(\\sigma_{\\varepsilon}^{2}\\) across the values of \\(x\\). The associations of the variables are given by \\[\\begin{equation*} y = \\alpha + \\beta x + \\varepsilon \\end{equation*}\\] where \\(\\alpha\\) is the expected value of \\(y\\) when \\(x = 0\\) \\(\\beta\\) is the unit change in \\(y\\) for unit change in \\(x\\) \\(\\alpha + \\beta\\) is the expected value of \\(y\\) when \\(x = 1\\) Figure 4.1: Student’s \\(t\\)-test 4.1 Symbolic Let \\(\\left\\{ y, x, \\varepsilon \\right\\}\\) be the variables of interest. \\[\\begin{align*}\\mathbf{A} &amp;=\\left( \\begin{array}{ccc} 0 &amp; \\beta &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{S} &amp;=\\left( \\begin{array}{ccc} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma _{x} ^{2} &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma _{\\varepsilon } ^{2} \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{C} &amp;=\\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\mathbf{S} \\left[ \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\right]^{\\mathsf{T}} \\\\\\\\ &amp;=\\mathbf{E} \\mathbf{S} \\mathbf{E}^{\\mathsf{T}} \\\\\\\\ &amp;=\\left( \\begin{array}{ccc} 1 &amp; \\beta &amp; 1 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array} \\right)\\left( \\begin{array}{ccc} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma _{x} ^{2} &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma _{\\varepsilon } ^{2} \\end{array} \\right)\\left( \\begin{array}{ccc} 1 &amp; \\beta &amp; 1 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array} \\right)^{\\mathsf{T}}\\\\ &amp;=\\left( \\begin{array}{ccc} \\sigma _{x} ^{2} \\beta ^{2} + \\sigma _{\\varepsilon } ^{2} &amp; \\beta \\sigma _{x} ^{2} &amp; \\sigma _{\\varepsilon } ^{2} \\\\ \\sigma _{x} ^{2} \\beta &amp; \\sigma _{x} ^{2} &amp; 0 \\\\ \\sigma _{\\varepsilon } ^{2} &amp; 0 &amp; \\sigma _{\\varepsilon } ^{2} \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{F} &amp;=\\left( \\begin{array}{ccc} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{M} &amp;=\\mathbf{F} \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\mathbf{S} \\left[ \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\right]^{\\mathsf{T}} \\mathbf{F}^{\\mathsf{T}}\\\\ &amp;=\\mathbf{F} \\mathbf{E} \\mathbf{S} \\mathbf{E}^{\\mathsf{T}} \\mathbf{F}^{\\mathsf{T}} \\\\\\\\ &amp;=\\mathbf{F} \\mathbf{C} \\mathbf{F}^{\\mathsf{T}} \\\\\\\\ &amp;=\\left( \\begin{array}{ccc} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\end{array} \\right)\\left( \\begin{array}{ccc} \\sigma _{x} ^{2} \\beta ^{2} + \\sigma _{\\varepsilon } ^{2} &amp; \\beta \\sigma _{x} ^{2} &amp; \\sigma _{\\varepsilon } ^{2} \\\\ \\sigma _{x} ^{2} \\beta &amp; \\sigma _{x} ^{2} &amp; 0 \\\\ \\sigma _{\\varepsilon } ^{2} &amp; 0 &amp; \\sigma _{\\varepsilon } ^{2} \\end{array} \\right)\\left( \\begin{array}{ccc} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\end{array} \\right)^{\\mathsf{T}} \\\\\\\\ &amp;=\\left( \\begin{array}{cc} \\sigma _{x} ^{2} \\beta ^{2} + \\sigma _{\\varepsilon } ^{2} &amp; \\beta \\sigma _{x} ^{2} \\\\ \\sigma _{x} ^{2} \\beta &amp; \\sigma _{x} ^{2} \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{v} &amp;=\\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\mathbf{u}\\\\ &amp;=\\left[\\left( \\begin{array}{ccc} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array} \\right)-\\left( \\begin{array}{ccc} 0 &amp; \\beta &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{array} \\right)\\right]^{\\mathsf{-1}}\\left( \\begin{array}{c} \\alpha \\\\ \\mu _{x} \\\\ 0 \\end{array} \\right)\\\\ &amp;=\\left( \\begin{array}{c} \\alpha + \\beta \\mu _{x} \\\\ \\mu _{x} \\\\ 0 \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{u} &amp;=\\left( \\mathbf{I} - \\mathbf{A} \\right) \\mathbf{v}\\\\ &amp;=\\left[\\left( \\begin{array}{ccc} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array} \\right)-\\left( \\begin{array}{ccc} 0 &amp; \\beta &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{array} \\right)\\right]\\left( \\begin{array}{c} \\alpha + \\beta \\mu _{x} \\\\ \\mu _{x} \\\\ 0 \\end{array} \\right)\\\\ &amp;=\\left( \\begin{array}{c} \\alpha \\\\ \\mu _{x} \\\\ 0 \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{g} &amp;=\\mathbf{F} \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\mathbf{u}\\\\ &amp;=\\left[\\left( \\begin{array}{ccc} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array} \\right)-\\left( \\begin{array}{ccc} 0 &amp; \\beta &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{array} \\right)\\right]^{-1}\\left( \\begin{array}{c} \\alpha \\\\ \\mu _{x} \\\\ 0 \\end{array} \\right)\\\\ &amp;=\\left( \\begin{array}{c} \\alpha + \\beta \\mu _{x} \\\\ \\mu _{x} \\end{array} \\right)\\end{align*}\\] 4.1.1 Using the ramR Package A ## [,1] [,2] [,3] ## [1,] &quot;0&quot; &quot;beta&quot; &quot;1&quot; ## [2,] &quot;0&quot; &quot;0&quot; &quot;0&quot; ## [3,] &quot;0&quot; &quot;0&quot; &quot;0&quot; S ## [,1] [,2] [,3] ## [1,] &quot;0&quot; &quot;0&quot; &quot;0&quot; ## [2,] &quot;0&quot; &quot;sigma[x]^2&quot; &quot;0&quot; ## [3,] &quot;0&quot; &quot;0&quot; &quot;sigma[varepsilon]^2&quot; u ## [,1] ## [1,] &quot;alpha&quot; ## [2,] &quot;mu[x]&quot; ## [3,] &quot;0&quot; filter ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 0 1 0 The covariance expectations can be symbolically derived using the ramR::C_sym() function. ramR::C_sym(A, S) ## {{sigma[x]^2*beta^2+sigma[varepsilon]^2, beta*sigma[x]^2, sigma[varepsilon]^2}, ## { sigma[x]^2*beta, sigma[x]^2, 0}, ## { sigma[varepsilon]^2, 0, sigma[varepsilon]^2}} \\[\\begin{equation*}\\mathbf{C} =\\left( \\begin{array}{ccc} \\sigma _{x} ^{2} \\beta ^{2} + \\sigma _{\\varepsilon } ^{2} &amp; \\beta \\sigma _{x} ^{2} &amp; \\sigma _{\\varepsilon } ^{2} \\\\ \\sigma _{x} ^{2} \\beta &amp; \\sigma _{x} ^{2} &amp; 0 \\\\ \\sigma _{\\varepsilon } ^{2} &amp; 0 &amp; \\sigma _{\\varepsilon } ^{2} \\end{array} \\right)\\end{equation*}\\] The covariance expectations for the observed variables can be symbolically derived using the ramR::M_sym() function. ramR::M_sym(A, S, filter) ## {{sigma[x]^2*beta^2+sigma[varepsilon]^2, beta*sigma[x]^2}, ## { sigma[x]^2*beta, sigma[x]^2}} \\[\\begin{equation*}\\mathbf{M} =\\left( \\begin{array}{cc} \\sigma _{x} ^{2} \\beta ^{2} + \\sigma _{\\varepsilon } ^{2} &amp; \\beta \\sigma _{x} ^{2} \\\\ \\sigma _{x} ^{2} \\beta &amp; \\sigma _{x} ^{2} \\end{array} \\right)\\end{equation*}\\] The mean expectations can be symbolically derived using the ramR::v_sym() function. ramR::v_sym(A, u) ## {{alpha+beta*mu[x]}, ## { mu[x]}, ## { 0}} \\[\\begin{equation*}\\mathbf{v} =\\left( \\begin{array}{c} \\alpha + \\beta \\mu _{x} \\\\ \\mu _{x} \\\\ 0 \\end{array} \\right)\\end{equation*}\\] The mean expectations for the observed variables can be symbolically derived using the ramR::g_sym() function. ramR::g_sym(A, u, filter) ## {{alpha+beta*mu[x]}, ## { mu[x]}} \\[\\begin{equation*}\\mathbf{g} =\\left( \\begin{array}{c} \\alpha + \\beta \\mu _{x} \\\\ \\mu _{x} \\end{array} \\right)\\end{equation*}\\] 4.2 Numerical Example head(df) ## y x ## 1 1.3709584 0 ## 2 -0.5646982 0 ## 3 0.3631284 0 ## 4 0.6328626 0 ## 5 0.4042683 0 ## 6 -0.1061245 0 summary(df) ## y x ## Min. :-4.6785 Min. :0.0 ## 1st Qu.:-0.2622 1st Qu.:0.0 ## Median : 0.5013 Median :0.5 ## Mean : 0.5000 Mean :0.5 ## 3rd Qu.: 1.2618 3rd Qu.:1.0 ## Max. : 5.7839 Max. :1.0 4.2.1 \\(t\\)-test t.test &lt;- t.test(y ~ x, data = df) t.test ## ## Welch Two Sample t-test ## ## data: y by x ## t = -706.06, df = 2e+06, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.0016565 -0.9961108 ## sample estimates: ## mean in group 0 mean in group 1 ## 0.0005737398 0.9994574009 t.test$estimate ## mean in group 0 mean in group 1 ## 0.0005737398 0.9994574009 4.2.2 Linear Regression lm &lt;- lm(y ~ x, data = df) summary(lm) ## ## Call: ## lm(formula = y ~ x, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.8838 -0.6745 0.0005 0.6749 4.8195 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.0005737 0.0010004 0.574 0.566 ## x 0.9988837 0.0014147 706.057 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1 on 1999998 degrees of freedom ## Multiple R-squared: 0.1995, Adjusted R-squared: 0.1995 ## F-statistic: 4.985e+05 on 1 and 1999998 DF, p-value: &lt; 2.2e-16 coef(lm) ## (Intercept) x ## 0.0005737398 0.9988836611 4.2.3 Structural Equation Modeling model &lt;- &quot; y ~ x y ~ 1 x ~ 1 &quot; fit &lt;- lavaan::sem(model, data = df) lavaan::summary(fit) ## lavaan 0.6-7 ended normally after 15 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 5 ## ## Number of observations 2000000 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## y ~ ## x 0.999 0.001 706.057 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .y 0.001 0.001 0.574 0.566 ## x 0.500 0.000 1414.214 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .y 1.001 0.001 1000.000 0.000 ## x 0.250 0.000 1000.000 0.000 lavaan::coef(fit) ## y~x y~1 x~1 y~~y x~~x ## 0.999 0.001 0.500 1.001 0.250 label parameter \\(\\alpha\\) 0 \\(\\beta\\) 1 \\(\\sigma^{2}_{x}\\) 0.25 \\(\\sigma^{2}_{\\varepsilon}\\) 0.25 \\(\\mu_x\\) 0.5 4.2.4 Using the ramR Package A ## y x e ## y 0 0.9988837 1 ## x 0 0.0000000 0 ## e 0 0.0000000 0 S ## y x e ## y 0 0.0000000 0.0000000 ## x 0 0.2500001 0.0000000 ## e 0 0.0000000 0.2494423 u ## [,1] ## y 0.0005737398 ## x 0.5000000000 ## e 0.0000000000 filter ## y x e ## y 1 0 0 ## x 0 1 0 The covariance expectations can be numerically derived using the ramR::C_num() function. ramR::C_num(A, S) ## y x e ## y 0.4988845 0.2497210 0.2494423 ## x 0.2497210 0.2500001 0.0000000 ## e 0.2494423 0.0000000 0.2494423 The covariance expectations for the observed variables can be numerically derived using the ramR::M_num() function. ramR::M_num(A, S, filter) ## y x ## y 0.4988845 0.2497210 ## x 0.2497210 0.2500001 The mean expectations can be numerically derived using the ramR::v_num() function. ramR::v_num(A, u) ## v ## y 0.5000156 ## x 0.5000000 ## e 0.0000000 The mean expectations for the observed variables can be numerically derived using the ramR::v_num() function. ramR::g_num(A, u, filter) ## g ## y 0.5000156 ## x 0.5000000 "],["references.html", "References", " References Boker, Steven M., and John J. McArdle. 2005. “Path Analysis and Path Diagrams.” In Encyclopedia of Statistics in Behavioral Science, edited by Brian S. Everitt and David C. Howell, 1529–31. Chichester, UK: John Wiley &amp; Sons, Ltd. https://doi.org/10.1002/0470013192.bsa471. McArdle, John J. 2005. “The Development of the RAM Rules for Latent Variable Structural Equation Modeling.” In Contemporary Psychometrics: A Festschrift for Roderick P. McDonald, edited by Albert Maydeu-Olivares and John J. McArdle, 225–73. Multivariate Applications Book Series. Mahwah, NJ: Lawrence Erlbaum Associates. McArdle, John J., and Roderick P. McDonald. 1984. “Some Algebraic Properties of the Reticular Action Model for Moment Structures.” British Journal of Mathematical and Statistical Psychology 37 (2): 234–51. https://doi.org/10.1111/j.2044-8317.1984.tb00802.x. Pesigan, Ivan Jacob Agaloos. 2021. ramR: Reticular Action Model (RAM) Notation. https://github.com/jeksterslab/ramR. "]]
