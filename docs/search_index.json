[["index.html", "Reticular Action Model (RAM) Notation Notes Chapter 1 Description", " Reticular Action Model (RAM) Notation Notes Ivan Jacob Agaloos Pesigan 2021-01-23 Chapter 1 Description This is a collection of my notes on the Reticular Action Model (RAM) notation that accompanies the ramR package (Pesigan 2021) in the R statistical environment (R Core Team 2020). You can install the released version of ramR from GitHub with: remotes::install_github(&quot;jeksterslab/ramR&quot;) These notes are based on the following resources: Boker and McArdle (2005) McArdle and McDonald (1984) McArdle (2005) See GitHub Pages for the html deployment. "],["ram-matrix-notation.html", "Chapter 2 Reticular Action Model (RAM) Matrix Notation 2.1 Full Model 2.2 Observed/Manifest/Given Variables vs. Unobserved/Latent/Hidden Variables", " Chapter 2 Reticular Action Model (RAM) Matrix Notation 2.1 Full Model Definition 2.1 \\[\\begin{equation} \\mathbf{v} = \\mathbf{A} \\mathbf{v} + \\mathbf{u} \\end{equation}\\] where \\(\\mathbf{v}\\) and \\(\\mathbf{u}\\) are \\(t \\times 1\\) vectors of random variables \\(\\mathbf{A}\\) is a \\(t \\times t\\) matrix of directed or asymmetric relationship from column variable \\(v_j\\) to row variable \\(v_i\\) \\(\\mathbf{A}\\) represent the regression of each of the \\(t\\) variables \\(\\mathbf{v}\\) on the other \\(t - 1\\) variables diagonal \\(a_{i,i}\\) is zero \\(u_i\\) represent the residual of \\(v_i\\) if all regression coefficients on other variables are zero, then the variable \\(v_i\\) is considered the same as its own residual \\(u_i\\) Definition 2.2 \\[\\begin{equation} \\mathbf{S} = \\mathbb{E} \\left\\{ \\mathbf{u} \\mathbf{u}^{\\prime} \\right\\} , \\end{equation}\\] where \\(\\mathbf{S}\\) is a \\(t \\times t\\) matrix of undirected or symmetric relationship the notation \\(\\boldsymbol{\\Omega}\\) is used in other sources for \\(\\mathbf{S}\\) \\(\\mathbb{E}\\) is the expectation operator Definition 2.3 \\[\\begin{equation} \\mathbf{C} = \\mathbb{E} \\left\\{ \\mathbf{v} \\mathbf{v}^{\\prime} \\right\\} , \\end{equation}\\] where \\(\\mathbf{C}\\) is a \\(t \\times t\\) variance-covariance matrix the notation \\(\\boldsymbol{\\Sigma}\\) is used in other sources for \\(\\mathbf{C}\\) Definition 2.4 \\[\\begin{equation*} \\mathbf{v} = \\mathbf{A} \\mathbf{v} + \\mathbf{u} \\end{equation*}\\] can be rewritten as \\[\\begin{equation} \\begin{split} \\mathbf{v} - \\mathbf{A} \\mathbf{v} &amp;= \\mathbf{u} \\\\ \\mathbf{u} &amp;= \\mathbf{v} - \\mathbf{A} \\mathbf{v} \\\\ \\mathbf{u} &amp;= \\left( \\mathbf{I} - \\mathbf{A} \\right) \\mathbf{v} \\end{split} \\end{equation}\\] assuming that \\(\\left( \\mathbf{I} - \\mathbf{A} \\right)\\) is non-singular, \\[\\begin{equation} \\mathbf{E} = \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\end{equation}\\] then \\[\\begin{equation} \\begin{split} \\mathbf{v} &amp;= \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\mathbf{u} \\\\ &amp;= \\mathbf{E} \\mathbf{u} . \\end{split} \\end{equation}\\] Using the definitions above, \\(\\mathbf{S}\\) and \\(\\mathbf{C}\\) are given by \\[\\begin{equation} \\begin{split} \\mathbf{S} &amp;= \\left( \\mathbf{I} - \\mathbf{A} \\right) \\mathbf{C} \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\\\ &amp;= \\mathbf{E}^{-1} \\mathbf{C} \\left( \\mathbf{E}^{-1} \\right)^{\\mathsf{T}} \\end{split} \\end{equation}\\] \\[\\begin{equation} \\begin{split} \\mathbf{C} &amp;= \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\mathbf{S} \\left[ \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\right]^{\\mathsf{T}} \\\\ &amp;= \\mathbf{E} \\mathbf{S} \\mathbf{E}^{\\mathsf{T}} \\end{split} \\end{equation}\\] 2.2 Observed/Manifest/Given Variables vs. Unobserved/Latent/Hidden Variables Definition 2.5 \\[\\begin{equation} \\mathbf{v} = \\begin{bmatrix} \\mathbf{g}_{p \\times 1} \\\\ \\mathbf{h}_{q \\times 1} \\\\ \\end{bmatrix} \\end{equation}\\] \\[\\begin{equation} t = p + q \\end{equation}\\] \\(\\mathbf{g}\\) may be considered observed, manifest or given variables \\(\\mathbf{h}\\) may be considered unobserved, latent, or hidden variables Definition 2.6 \\[\\begin{equation} \\mathbf{F} = \\begin{bmatrix} \\mathbf{I}_{p \\times p} : \\mathbf{0}_{p \\times q} \\end{bmatrix} \\end{equation}\\] the \\(\\mathbf{F}\\) matrix acts as a filter to select the manifest variables out of the full set of manifest and latent variables \\[\\begin{equation} \\mathbf{g} = \\mathbf{F} \\mathbf{v} \\end{equation}\\] \\[\\begin{equation} \\begin{split} \\mathbf{g} &amp;= \\mathbf{F} \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\mathbf{u} \\\\ &amp;= \\mathbf{F} \\mathbf{E} \\mathbf{u} \\end{split} \\end{equation}\\] Definition 2.7 \\[\\begin{equation} \\mathbf{M} = \\mathbb{E} \\left\\{ \\mathbf{g} \\mathbf{g}^{\\mathsf{T}} \\right\\} \\end{equation}\\] \\[\\begin{equation} \\begin{split} \\mathbf{M} &amp;= \\mathbf{F} \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\mathbf{S} \\left[ \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\right]^{\\mathsf{T}} \\mathbf{F}^{\\mathsf{T}} \\\\ &amp;= \\mathbf{F} \\mathbf{E} \\mathbf{S} \\mathbf{E}^{\\mathsf{T}} \\mathbf{F}^{\\mathsf{T}} \\\\ &amp;= \\mathbf{F} \\mathbf{C} \\mathbf{F}^{\\mathsf{T}} \\end{split} \\end{equation}\\] when components of \\(\\mathbf{v}\\) are permuted, the columns of \\(\\mathbf{F}\\) can be correspondingly permuted the rows and columns of \\(\\mathbf{C}\\) that are filtered out by \\(\\mathbf{F}\\) contain useful information about the latent variable structure. The equations above completely define RAM. "],["ram-diagram.html", "Chapter 3 Reticular Action Model (RAM) Path Diagram", " Chapter 3 Reticular Action Model (RAM) Path Diagram Figure 3.1: Path Diagram Elements Figure 3.2: Two-Variable Regression Model Figure 3.3: \\(k\\)-Variable Regression Model Figure 3.4: Two-Factor Confirmatory Factor Analysis Model Figure 3.5: Two-Factor Confirmatory Factor Analysis Model with Mean Structure Figure 3.6: Path Model with Latent Variables "],["ram-t.html", "Chapter 4 Student’s \\(t\\)-test 4.1 Symbolic 4.2 Numerical Example 4.3 Equations to RAM 4.4 Equations to Expectations", " Chapter 4 Student’s \\(t\\)-test In this section, the Student’s \\(t\\)-test is presented as a structural equation model using the RAM notation. Let \\(y\\) be a continuous dependent variable, \\(x\\) be a dichotomous independent variable \\(\\left( x = \\{0, 1\\} \\right)\\), and \\(\\varepsilon\\) be the stochastic error term with mean 0 and constant variance of \\(\\sigma_{\\varepsilon}^{2}\\) across the values of \\(x\\). The associations of the variables are given by \\[\\begin{equation*} y = \\alpha + \\beta x + \\varepsilon \\end{equation*}\\] where \\(\\alpha\\) is the expected value of \\(y\\) when \\(x = 0\\) \\(\\beta\\) is the unit change in \\(y\\) for unit change in \\(x\\) \\(\\alpha + \\beta\\) is the expected value of \\(y\\) when \\(x = 1\\) Figure 4.1: Student’s \\(t\\)-test 4.1 Symbolic Let \\(\\left\\{ y, x, \\varepsilon \\right\\}\\) be the variables of interest. \\[\\begin{align*}\\mathbf{A} &amp;=\\left( \\begin{array}{ccc} 0 &amp; \\beta &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{S} &amp;=\\left( \\begin{array}{ccc} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma _{x} ^{2} &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma _{\\varepsilon } ^{2} \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{C} &amp;=\\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\mathbf{S} \\left[ \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\right]^{\\mathsf{T}} \\\\\\\\ &amp;=\\mathbf{E} \\mathbf{S} \\mathbf{E}^{\\mathsf{T}} \\\\\\\\ &amp;=\\left( \\begin{array}{ccc} 1 &amp; \\beta &amp; 1 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array} \\right)\\left( \\begin{array}{ccc} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma _{x} ^{2} &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma _{\\varepsilon } ^{2} \\end{array} \\right)\\left( \\begin{array}{ccc} 1 &amp; \\beta &amp; 1 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array} \\right)^{\\mathsf{T}}\\\\ &amp;=\\left( \\begin{array}{ccc} \\sigma _{x} ^{2} \\beta ^{2} + \\sigma _{\\varepsilon } ^{2} &amp; \\beta \\sigma _{x} ^{2} &amp; \\sigma _{\\varepsilon } ^{2} \\\\ \\sigma _{x} ^{2} \\beta &amp; \\sigma _{x} ^{2} &amp; 0 \\\\ \\sigma _{\\varepsilon } ^{2} &amp; 0 &amp; \\sigma _{\\varepsilon } ^{2} \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{F} &amp;=\\left( \\begin{array}{ccc} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{M} &amp;=\\mathbf{F} \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\mathbf{S} \\left[ \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\right]^{\\mathsf{T}} \\mathbf{F}^{\\mathsf{T}}\\\\ &amp;=\\mathbf{F} \\mathbf{E} \\mathbf{S} \\mathbf{E}^{\\mathsf{T}} \\mathbf{F}^{\\mathsf{T}} \\\\\\\\ &amp;=\\mathbf{F} \\mathbf{C} \\mathbf{F}^{\\mathsf{T}} \\\\\\\\ &amp;=\\left( \\begin{array}{ccc} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\end{array} \\right)\\left( \\begin{array}{ccc} \\sigma _{x} ^{2} \\beta ^{2} + \\sigma _{\\varepsilon } ^{2} &amp; \\beta \\sigma _{x} ^{2} &amp; \\sigma _{\\varepsilon } ^{2} \\\\ \\sigma _{x} ^{2} \\beta &amp; \\sigma _{x} ^{2} &amp; 0 \\\\ \\sigma _{\\varepsilon } ^{2} &amp; 0 &amp; \\sigma _{\\varepsilon } ^{2} \\end{array} \\right)\\left( \\begin{array}{ccc} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\end{array} \\right)^{\\mathsf{T}} \\\\\\\\ &amp;=\\left( \\begin{array}{cc} \\sigma _{x} ^{2} \\beta ^{2} + \\sigma _{\\varepsilon } ^{2} &amp; \\beta \\sigma _{x} ^{2} \\\\ \\sigma _{x} ^{2} \\beta &amp; \\sigma _{x} ^{2} \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{v} &amp;=\\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\mathbf{u}\\\\ &amp;=\\left[\\left( \\begin{array}{ccc} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array} \\right)-\\left( \\begin{array}{ccc} 0 &amp; \\beta &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{array} \\right)\\right]^{\\mathsf{-1}}\\left( \\begin{array}{c} \\alpha \\\\ \\mu _{x} \\\\ 0 \\end{array} \\right)\\\\ &amp;=\\left( \\begin{array}{c} \\alpha + \\beta \\mu _{x} \\\\ \\mu _{x} \\\\ 0 \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{u} &amp;=\\left( \\mathbf{I} - \\mathbf{A} \\right) \\mathbf{v}\\\\ &amp;=\\left[\\left( \\begin{array}{ccc} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array} \\right)-\\left( \\begin{array}{ccc} 0 &amp; \\beta &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{array} \\right)\\right]\\left( \\begin{array}{c} \\alpha + \\beta \\mu _{x} \\\\ \\mu _{x} \\\\ 0 \\end{array} \\right)\\\\ &amp;=\\left( \\begin{array}{c} \\alpha \\\\ \\mu _{x} \\\\ 0 \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{g} &amp;=\\mathbf{F} \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\mathbf{u}\\\\ &amp;=\\left[\\left( \\begin{array}{ccc} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array} \\right)-\\left( \\begin{array}{ccc} 0 &amp; \\beta &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{array} \\right)\\right]^{-1}\\left( \\begin{array}{c} \\alpha \\\\ \\mu _{x} \\\\ 0 \\end{array} \\right)\\\\ &amp;=\\left( \\begin{array}{c} \\alpha + \\beta \\mu _{x} \\\\ \\mu _{x} \\end{array} \\right)\\end{align*}\\] 4.1.1 Using the ramR Package A ## y x e ## y &quot;0&quot; &quot;beta&quot; &quot;1&quot; ## x &quot;0&quot; &quot;0&quot; &quot;0&quot; ## e &quot;0&quot; &quot;0&quot; &quot;0&quot; S ## y x e ## y &quot;0&quot; &quot;0&quot; &quot;0&quot; ## x &quot;0&quot; &quot;sigma[x]^2&quot; &quot;0&quot; ## e &quot;0&quot; &quot;0&quot; &quot;sigma[varepsilon]^2&quot; u ## u ## y &quot;alpha&quot; ## x &quot;mu[x]&quot; ## e &quot;0&quot; filter ## y x e ## y 1 0 0 ## x 0 1 0 The covariance expectations can be symbolically derived using the ramR::C_sym() function. ramR::C_sym(A, S) ## {{sigma[x]^2*beta^2+sigma[varepsilon]^2, beta*sigma[x]^2, sigma[varepsilon]^2}, ## { sigma[x]^2*beta, sigma[x]^2, 0}, ## { sigma[varepsilon]^2, 0, sigma[varepsilon]^2}} \\[\\begin{equation*}\\mathbf{C} =\\left( \\begin{array}{ccc} \\sigma _{x} ^{2} \\beta ^{2} + \\sigma _{\\varepsilon } ^{2} &amp; \\beta \\sigma _{x} ^{2} &amp; \\sigma _{\\varepsilon } ^{2} \\\\ \\sigma _{x} ^{2} \\beta &amp; \\sigma _{x} ^{2} &amp; 0 \\\\ \\sigma _{\\varepsilon } ^{2} &amp; 0 &amp; \\sigma _{\\varepsilon } ^{2} \\end{array} \\right)\\end{equation*}\\] The covariance expectations for the observed variables can be symbolically derived using the ramR::M_sym() function. ramR::M_sym(A, S, filter) ## {{sigma[x]^2*beta^2+sigma[varepsilon]^2, beta*sigma[x]^2}, ## { sigma[x]^2*beta, sigma[x]^2}} \\[\\begin{equation*}\\mathbf{M} =\\left( \\begin{array}{cc} \\sigma _{x} ^{2} \\beta ^{2} + \\sigma _{\\varepsilon } ^{2} &amp; \\beta \\sigma _{x} ^{2} \\\\ \\sigma _{x} ^{2} \\beta &amp; \\sigma _{x} ^{2} \\end{array} \\right)\\end{equation*}\\] The mean expectations can be symbolically derived using the ramR::v_sym() function. ramR::v_sym(A, u) ## {{alpha+beta*mu[x]}, ## { mu[x]}, ## { 0}} \\[\\begin{equation*}\\mathbf{v} =\\left( \\begin{array}{c} \\alpha + \\beta \\mu _{x} \\\\ \\mu _{x} \\\\ 0 \\end{array} \\right)\\end{equation*}\\] The mean expectations for the observed variables can be symbolically derived using the ramR::g_sym() function. ramR::g_sym(A, u, filter) ## {{alpha+beta*mu[x]}, ## { mu[x]}} \\[\\begin{equation*}\\mathbf{g} =\\left( \\begin{array}{c} \\alpha + \\beta \\mu _{x} \\\\ \\mu _{x} \\end{array} \\right)\\end{equation*}\\] 4.2 Numerical Example Let df be a random sample with the following parameters Parameter \\(x = 0\\) \\(x = 1\\) Sample Size 500 500 \\(\\mu\\) 0 1 \\(\\sigma^2\\) 1 1 Parameter Description Estimate \\(\\alpha\\) Mean of \\(x = 0\\). 0 \\(\\beta\\) Mean of \\(x = 1\\) minus \\(x = 0\\). 1 head(df) ## y x ## 1 1.3709584 0 ## 2 -0.5646982 0 ## 3 0.3631284 0 ## 4 0.6328626 0 ## 5 0.4042683 0 ## 6 -0.1061245 0 summary(df) ## y x ## Min. :-2.9931 Min. :0.0 ## 1st Qu.:-0.2770 1st Qu.:0.0 ## Median : 0.4503 Median :0.5 ## Mean : 0.4742 Mean :0.5 ## 3rd Qu.: 1.2492 3rd Qu.:1.0 ## Max. : 4.4953 Max. :1.0 4.2.1 \\(t\\)-test t.test(y ~ x, data = df) ## ## Welch Two Sample t-test ## ## data: y by x ## t = -15.897, df = 994.36, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.1329278 -0.8839594 ## sample estimates: ## mean in group 0 mean in group 1 ## -0.03004622 0.97839737 4.2.2 Linear Regression summary(lm(y ~ x, data = df)) ## ## Call: ## lm(formula = y ~ x, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.3501 -0.6517 0.0086 0.6858 3.5169 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.03005 0.04486 -0.67 0.503 ## x 1.00844 0.06344 15.90 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.003 on 998 degrees of freedom ## Multiple R-squared: 0.2021, Adjusted R-squared: 0.2013 ## F-statistic: 252.7 on 1 and 998 DF, p-value: &lt; 2.2e-16 4.2.3 Structural Equation Modeling 4.2.3.1 lavaan (Rosseel 2012) model &lt;- &quot; y ~ x &quot; fit &lt;- lavaan::sem( model, data = df, meanstructure = TRUE, fixed.x = FALSE ) lavaan::summary(fit) ## lavaan 0.6-7 ended normally after 12 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 5 ## ## Number of observations 1000 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## y ~ ## x 1.008 0.063 15.913 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .y -0.030 0.045 -0.671 0.503 ## x 0.500 0.016 31.623 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .y 1.004 0.045 22.361 0.000 ## x 0.250 0.011 22.361 0.000 4.2.3.2 OpenMx (Boker et al. 2020) RAM matrices can be used to specify models in OpenMx. Note, however, that the u vector in the RAM notation is M in the OpenMx notation. mxData &lt;- OpenMx::mxData( observed = df, type = &quot;raw&quot; ) mxA &lt;- OpenMx::mxMatrix( type = &quot;Full&quot;, nrow = 3, ncol = 3, free = c( F, T, F, F, F, F, F, F, F ), values = c( 0, 0.20, 1, 0, 0, 0, 0, 0, 0 ), labels = c( NA, &quot;beta&quot;, NA, NA, NA, NA, NA, NA, NA ), byrow = TRUE, name = &quot;mxA&quot; ) mxS &lt;- OpenMx::mxMatrix( type = &quot;Symm&quot;, nrow = 3, ncol = 3, free = c( F, F, F, F, T, F, F, F, T ), values = c( 0, 0, 0, 0, 0.20, 0, 0, 0, 0.20 ), labels = c( NA, NA, NA, NA, &quot;sigma2x&quot;, NA, NA, NA, &quot;sigma2e&quot; ), byrow = TRUE, name = &quot;mxS&quot; ) mxM &lt;- OpenMx::mxMatrix( type = &quot;Full&quot;, nrow = 1, ncol = 3, free = c( T, T, F ), values = c( 0.20, 0.20, 0 ), labels = c( &quot;alpha&quot;, &quot;mux&quot;, NA ), byrow = TRUE, name = &quot;mxM&quot; ) mxF &lt;- OpenMx::mxMatrix( type = &quot;Full&quot;, nrow = 2, ncol = 3, free = FALSE, values = c( 1, 0, 0, 0, 1, 0 ), byrow = TRUE, name = &quot;mxF&quot; ) expRAM &lt;- OpenMx::mxExpectationRAM( A = &quot;mxA&quot;, S = &quot;mxS&quot;, F = &quot;mxF&quot;, M = &quot;mxM&quot;, dimnames = c( &quot;y&quot;, &quot;x&quot;, &quot;e&quot; ) ) objML &lt;- OpenMx::mxFitFunctionML() mxMod &lt;- OpenMx::mxModel( name = &quot;Student&#39;s t test&quot;, data = mxData, matrices = list( mxA, mxS, mxF, mxM ), expectation = expRAM, fitfunction = objML ) fit &lt;- OpenMx::mxRun(mxMod) ## Running Student&#39;s t test with 5 parameters summary(fit) ## Summary of Student&#39;s t test ## ## free parameters: ## name matrix row col Estimate Std.Error A ## 1 beta mxA 1 2 1.00844356 0.06337369 ## 2 sigma2x mxS 2 2 0.25000000 0.01118034 ## 3 sigma2e mxS 3 3 1.00402596 0.04490152 ## 4 alpha mxM 1 y -0.03004621 0.04481202 ## 5 mux mxM 1 x 0.49999999 0.01581140 ## ## Model Statistics: ## | Parameters | Degrees of Freedom | Fit (-2lnL units) ## Model: 5 1995 4293.478 ## Saturated: 5 1995 NA ## Independence: 4 1996 NA ## Number of observations/statistics: 1000/2000 ## ## Information Criteria: ## | df Penalty | Parameters Penalty | Sample-Size Adjusted ## AIC: 303.4776 4303.478 4303.538 ## BIC: -9487.4941 4328.016 4312.136 ## CFI: NA ## TLI: 1 (also known as NNFI) ## RMSEA: 0 [95% CI (NA, NA)] ## Prob(RMSEA &lt;= 0.05): NA ## To get additional fit indices, see help(mxRefModels) ## timestamp: 2021-01-23 23:50:37 ## Wall clock time: 0.03900671 secs ## optimizer: SLSQP ## OpenMx version number: 2.18.1 ## Need help? See help(mxSummary) 4.2.4 Using the ramR Package A ## y x e ## y 0 1.008444 1 ## x 0 0.000000 0 ## e 0 0.000000 0 S ## y x e ## y 0 0.0000000 0.000000 ## x 0 0.2502503 0.000000 ## e 0 0.0000000 1.006038 u ## u ## y -0.03004622 ## x 0.50000000 ## e 0.00000000 filter ## y x e ## y 1 0 0 ## x 0 1 0 The covariance expectations can be numerically derived using the ramR::C_num() function. ramR::C_num(A, S) ## y x e ## y 1.2605321 0.2523633 1.006038 ## x 0.2523633 0.2502503 0.000000 ## e 1.0060380 0.0000000 1.006038 The covariance expectations for the observed variables can be numerically derived using the ramR::M_num() function. ramR::M_num(A, S, filter) ## y x ## y 1.2605321 0.2523633 ## x 0.2523633 0.2502503 The mean expectations can be numerically derived using the ramR::v_num() function. ramR::v_num(A, u) ## v ## y 0.4741756 ## x 0.5000000 ## e 0.0000000 The mean expectations for the observed variables can be numerically derived using the ramR::v_num() function. ramR::g_num(A, u, filter) ## g ## y 0.4741756 ## x 0.5000000 4.3 Equations to RAM The ramR package has a utility function to convert structural equations to RAM notation. The Student’s \\(t\\)-test can be expressed in the following equations eq &lt;- &quot; # VARIABLE1 OPERATION VARIABLE2 LABEL e by y 1 y on x beta e with e sigma[varepsilon]^2 x with x sigma[x]^2 y on 1 alpha x on 1 mu[x] &quot; Figure 4.2: Student’s \\(t\\)-test’s Structural Equations The error term is treated as a latent variable and defined with the operation by. Its value is constrained to \\(1\\). The regression of \\(y\\) on \\(x\\) is defined by operation on. It is labeled as beta. The variance of \\(x\\) and the error variance are defined using the operation with. These are labeled sigma[x]^2 and sigma[varepsilon]^2 respectively. The intercept and the mean of \\(x\\) are defined using the operation on 1. These are labeled alpha and mu[x] respectively. The ramR::eq2ram converts the equations to RAM notation. ramR::eq2ram(eq) ## $eq ## var1 op var2 label ## 1 e by y 1 ## 2 y on x beta ## 3 e with e sigma[varepsilon]^2 ## 4 x with x sigma[x]^2 ## 5 y on 1 alpha ## 6 x on 1 mu[x] ## ## $variables ## [1] &quot;y&quot; &quot;x&quot; &quot;e&quot; ## ## $A ## y x e ## y &quot;0&quot; &quot;beta&quot; &quot;1&quot; ## x &quot;0&quot; &quot;0&quot; &quot;0&quot; ## e &quot;0&quot; &quot;0&quot; &quot;0&quot; ## ## $S ## y x e ## y &quot;0&quot; &quot;0&quot; &quot;0&quot; ## x &quot;0&quot; &quot;sigma[x]^2&quot; &quot;0&quot; ## e &quot;0&quot; &quot;0&quot; &quot;sigma[varepsilon]^2&quot; ## ## $filter ## y x e ## y 1 0 0 ## x 0 1 0 ## ## $u ## u ## y &quot;alpha&quot; ## x &quot;mu[x]&quot; ## e &quot;0&quot; 4.4 Equations to Expectations The ramR package has a utility function to convert structural equations to expectations both symbolically and numerically. eq &lt;- &quot; # VARIABLE1 OPERATION VARIABLE2 LABEL e by y 1 y on x beta e with e sigma[varepsilon]^2 x with x sigma[x]^2 y on 1 alpha x on 1 mu[x] &quot; ramR::eq2exp_sym(eq) ## $variables ## [1] &quot;y&quot; &quot;x&quot; &quot;e&quot; ## ## $A ## {{ 0, beta, 1}, ## { 0, 0, 0}, ## { 0, 0, 0}} ## ## $S ## {{ 0, 0, 0}, ## { 0, sigma[x]^2, 0}, ## { 0, 0, sigma[varepsilon]^2}} ## ## $u ## {{alpha}, ## {mu[x]}, ## { 0}} ## ## $filter ## {{1, 0, 0}, ## {0, 1, 0}} ## ## $v ## {{alpha+beta*mu[x]}, ## { mu[x]}, ## { 0}} ## ## $g ## {{alpha+beta*mu[x]}, ## { mu[x]}} ## ## $C ## {{sigma[x]^2*beta^2+sigma[varepsilon]^2, beta*sigma[x]^2, sigma[varepsilon]^2}, ## { sigma[x]^2*beta, sigma[x]^2, 0}, ## { sigma[varepsilon]^2, 0, sigma[varepsilon]^2}} ## ## $M ## {{sigma[x]^2*beta^2+sigma[varepsilon]^2, beta*sigma[x]^2}, ## { sigma[x]^2*beta, sigma[x]^2}} eq &lt;- &quot; # VARIABLE1 OPERATION VARIABLE2 VALUE e by y 1.00 y on x 1.00 e with e 1.00 x with x 0.25 y on 1 0.00 x on 1 0.50 &quot; ramR::eq2exp_num(eq) ## $variables ## [1] &quot;y&quot; &quot;x&quot; &quot;e&quot; ## ## $A ## y x e ## y 0 1 1 ## x 0 0 0 ## e 0 0 0 ## ## $S ## y x e ## y 0 0.00 0 ## x 0 0.25 0 ## e 0 0.00 1 ## ## $u ## u ## y 0.0 ## x 0.5 ## e 0.0 ## ## $filter ## y x e ## y 1 0 0 ## x 0 1 0 ## ## $v ## v ## y 0.5 ## x 0.5 ## e 0.0 ## ## $g ## g ## y 0.5 ## x 0.5 ## ## $C ## y x e ## y 1.25 0.25 1 ## x 0.25 0.25 0 ## e 1.00 0.00 1 ## ## $M ## y x ## y 1.25 0.25 ## x 0.25 0.25 "],["ram-anova.html", "Chapter 5 One-Way Analysis of Variance 5.1 Symbolic 5.2 Numerical Example 5.3 Equations to RAM 5.4 Equations to Expectations", " Chapter 5 One-Way Analysis of Variance In this section, one-way analysis of variance is presented as a structural equation model using the RAM notation. Let \\(y\\) be a continuous dependent variable, \\(x\\) be a categorical independent variable with three levels \\(\\left( x = \\{0, 1, 2\\} \\right)\\). The dependent variable \\(x\\) can be dummy coded as \\(x\\) \\(x_1\\) \\(x_2\\) \\(x = 0\\) 0 0 \\(x = 1\\) 1 0 \\(x = 2\\) 0 1 The associations of the variables are given by \\[\\begin{equation*} y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon \\end{equation*}\\] where \\(\\beta_0\\) is the expected value of \\(y\\) when \\(x = 0\\) \\(\\beta_1\\) is the unit change in \\(y\\) for unit change in \\(x_1\\) while \\(x_2\\) is constant \\(\\beta_2\\) is the unit change in \\(y\\) for unit change in \\(x_2\\) while \\(x_1\\) is constant \\(\\beta_0 + \\beta_1\\) is the expected value of \\(y\\) when \\(x = 1\\) \\(\\beta_0 + \\beta_2\\) is the expected value of \\(y\\) when \\(x = 2\\) Figure 5.1: One-Way Analysis of Variance 5.1 Symbolic Let \\(\\left\\{ y, x_1, x_2, \\varepsilon \\right\\}\\) be the variables of interest. \\[\\begin{align*}\\mathbf{A} &amp;=\\left( \\begin{array}{cccc} 0 &amp; \\beta _{1} &amp; \\beta _{2} &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{S} &amp;=\\left( \\begin{array}{cccc} 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma _{x_{1}} ^{2} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma _{x_{2}} ^{2} &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\sigma _{\\varepsilon } ^{2} \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{C} &amp;=\\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\mathbf{S} \\left[ \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\right]^{\\mathsf{T}} \\\\\\\\ &amp;=\\mathbf{E} \\mathbf{S} \\mathbf{E}^{\\mathsf{T}} \\\\\\\\ &amp;=\\left( \\begin{array}{cccc} 1 &amp; \\beta _{1} &amp; \\beta _{2} &amp; 1 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{array} \\right)\\left( \\begin{array}{cccc} 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma _{x_{1}} ^{2} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma _{x_{2}} ^{2} &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\sigma _{\\varepsilon } ^{2} \\end{array} \\right)\\left( \\begin{array}{cccc} 1 &amp; \\beta _{1} &amp; \\beta _{2} &amp; 1 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{array} \\right)^{\\mathsf{T}}\\\\ &amp;=\\left( \\begin{array}{cccc} \\sigma _{x_{1}} ^{2} \\beta _{1} ^{2} + \\sigma _{x_{2}} ^{2} \\beta _{2} ^{2} + \\sigma _{\\varepsilon } ^{2} &amp; \\beta _{1} \\sigma _{x_{1}} ^{2} &amp; \\beta _{2} \\sigma _{x_{2}} ^{2} &amp; \\sigma _{\\varepsilon } ^{2} \\\\ \\sigma _{x_{1}} ^{2} \\beta _{1} &amp; \\sigma _{x_{1}} ^{2} &amp; 0 &amp; 0 \\\\ \\sigma _{x_{2}} ^{2} \\beta _{2} &amp; 0 &amp; \\sigma _{x_{2}} ^{2} &amp; 0 \\\\ \\sigma _{\\varepsilon } ^{2} &amp; 0 &amp; 0 &amp; \\sigma _{\\varepsilon } ^{2} \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{F} &amp;=\\left( \\begin{array}{cccc} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{M} &amp;=\\mathbf{F} \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\mathbf{S} \\left[ \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\right]^{\\mathsf{T}} \\mathbf{F}^{\\mathsf{T}}\\\\ &amp;=\\mathbf{F} \\mathbf{E} \\mathbf{S} \\mathbf{E}^{\\mathsf{T}} \\mathbf{F}^{\\mathsf{T}} \\\\\\\\ &amp;=\\mathbf{F} \\mathbf{C} \\mathbf{F}^{\\mathsf{T}} \\\\\\\\ &amp;=\\left( \\begin{array}{cccc} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\end{array} \\right)\\left( \\begin{array}{cccc} \\sigma _{x_{1}} ^{2} \\beta _{1} ^{2} + \\sigma _{x_{2}} ^{2} \\beta _{2} ^{2} + \\sigma _{\\varepsilon } ^{2} &amp; \\beta _{1} \\sigma _{x_{1}} ^{2} &amp; \\beta _{2} \\sigma _{x_{2}} ^{2} &amp; \\sigma _{\\varepsilon } ^{2} \\\\ \\sigma _{x_{1}} ^{2} \\beta _{1} &amp; \\sigma _{x_{1}} ^{2} &amp; 0 &amp; 0 \\\\ \\sigma _{x_{2}} ^{2} \\beta _{2} &amp; 0 &amp; \\sigma _{x_{2}} ^{2} &amp; 0 \\\\ \\sigma _{\\varepsilon } ^{2} &amp; 0 &amp; 0 &amp; \\sigma _{\\varepsilon } ^{2} \\end{array} \\right)\\left( \\begin{array}{cccc} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\end{array} \\right)^{\\mathsf{T}} \\\\\\\\ &amp;=\\left( \\begin{array}{ccc} \\sigma _{x_{1}} ^{2} \\beta _{1} ^{2} + \\sigma _{x_{2}} ^{2} \\beta _{2} ^{2} + \\sigma _{\\varepsilon } ^{2} &amp; \\beta _{1} \\sigma _{x_{1}} ^{2} &amp; \\beta _{2} \\sigma _{x_{2}} ^{2} \\\\ \\sigma _{x_{1}} ^{2} \\beta _{1} &amp; \\sigma _{x_{1}} ^{2} &amp; 0 \\\\ \\sigma _{x_{2}} ^{2} \\beta _{2} &amp; 0 &amp; \\sigma _{x_{2}} ^{2} \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{v} &amp;=\\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\mathbf{u}\\\\ &amp;=\\left[\\left( \\begin{array}{cccc} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{array} \\right)-\\left( \\begin{array}{cccc} 0 &amp; \\beta _{1} &amp; \\beta _{2} &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{array} \\right)\\right]^{\\mathsf{-1}}\\left( \\begin{array}{c} \\beta _{0} \\\\ \\mu _{x_{1}} \\\\ \\mu _{x_{2}} \\\\ 0 \\end{array} \\right)\\\\ &amp;=\\left( \\begin{array}{c} \\beta _{0} + \\beta _{1} \\mu _{x_{1}} + \\beta _{2} \\mu _{x_{2}} \\\\ \\mu _{x_{1}} \\\\ \\mu _{x_{2}} \\\\ 0 \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{u} &amp;=\\left( \\mathbf{I} - \\mathbf{A} \\right) \\mathbf{v}\\\\ &amp;=\\left[\\left( \\begin{array}{cccc} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{array} \\right)-\\left( \\begin{array}{cccc} 0 &amp; \\beta _{1} &amp; \\beta _{2} &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{array} \\right)\\right]\\left( \\begin{array}{c} \\beta _{0} + \\beta _{1} \\mu _{x_{1}} + \\beta _{2} \\mu _{x_{2}} \\\\ \\mu _{x_{1}} \\\\ \\mu _{x_{2}} \\\\ 0 \\end{array} \\right)\\\\ &amp;=\\left( \\begin{array}{c} \\beta _{0} \\\\ \\mu _{x_{1}} \\\\ \\mu _{x_{2}} \\\\ 0 \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{g} &amp;=\\mathbf{F} \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\mathbf{u}\\\\ &amp;=\\left[\\left( \\begin{array}{cccc} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{array} \\right)-\\left( \\begin{array}{cccc} 0 &amp; \\beta _{1} &amp; \\beta _{2} &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{array} \\right)\\right]^{-1}\\left( \\begin{array}{c} \\beta _{0} \\\\ \\mu _{x_{1}} \\\\ \\mu _{x_{2}} \\\\ 0 \\end{array} \\right)\\\\ &amp;=\\left( \\begin{array}{c} \\beta _{0} + \\beta _{1} \\mu _{x_{1}} + \\beta _{2} \\mu _{x_{2}} \\\\ \\mu _{x_{1}} \\\\ \\mu _{x_{2}} \\end{array} \\right)\\end{align*}\\] 5.1.1 Using the ramR Package A ## y x1 x2 e ## y &quot;0&quot; &quot;beta[1]&quot; &quot;beta[2]&quot; &quot;1&quot; ## x1 &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## x2 &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## e &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; S ## y x1 x2 e ## y &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## x1 &quot;0&quot; &quot;sigma[x1]^2&quot; &quot;0&quot; &quot;0&quot; ## x2 &quot;0&quot; &quot;0&quot; &quot;sigma[x2]^2&quot; &quot;0&quot; ## e &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;sigma[varepsilon]^2&quot; u ## u ## y &quot;beta[0]&quot; ## x1 &quot;mu[x1]&quot; ## x2 &quot;mu[x2]&quot; ## e &quot;0&quot; filter ## y x1 x2 e ## y 1 0 0 0 ## x1 0 1 0 0 ## x2 0 0 1 0 The covariance expectations can be symbolically derived using the ramR::C_sym() function. ramR::C_sym(A, S) ## {{sigma[x1]^2*beta[1]^2+sigma[x2]^2*beta[2]^2+sigma[varepsilon]^2, beta[1]*sigma[x1]^2, beta[2]*sigma[x2]^2, sigma[varepsilon]^2}, ## { sigma[x1]^2*beta[1], sigma[x1]^2, 0, 0}, ## { sigma[x2]^2*beta[2], 0, sigma[x2]^2, 0}, ## { sigma[varepsilon]^2, 0, 0, sigma[varepsilon]^2}} \\[\\begin{equation*}\\mathbf{C} =\\left( \\begin{array}{cccc} \\sigma _{x_{1}} ^{2} \\beta _{1} ^{2} + \\sigma _{x_{2}} ^{2} \\beta _{2} ^{2} + \\sigma _{\\varepsilon } ^{2} &amp; \\beta _{1} \\sigma _{x_{1}} ^{2} &amp; \\beta _{2} \\sigma _{x_{2}} ^{2} &amp; \\sigma _{\\varepsilon } ^{2} \\\\ \\sigma _{x_{1}} ^{2} \\beta _{1} &amp; \\sigma _{x_{1}} ^{2} &amp; 0 &amp; 0 \\\\ \\sigma _{x_{2}} ^{2} \\beta _{2} &amp; 0 &amp; \\sigma _{x_{2}} ^{2} &amp; 0 \\\\ \\sigma _{\\varepsilon } ^{2} &amp; 0 &amp; 0 &amp; \\sigma _{\\varepsilon } ^{2} \\end{array} \\right)\\end{equation*}\\] The covariance expectations for the observed variables can be symbolically derived using the ramR::M_sym() function. ramR::M_sym(A, S, filter) ## {{sigma[x1]^2*beta[1]^2+sigma[x2]^2*beta[2]^2+sigma[varepsilon]^2, beta[1]*sigma[x1]^2, beta[2]*sigma[x2]^2}, ## { sigma[x1]^2*beta[1], sigma[x1]^2, 0}, ## { sigma[x2]^2*beta[2], 0, sigma[x2]^2}} \\[\\begin{equation*}\\mathbf{M} =\\left( \\begin{array}{ccc} \\sigma _{x_{1}} ^{2} \\beta _{1} ^{2} + \\sigma _{x_{2}} ^{2} \\beta _{2} ^{2} + \\sigma _{\\varepsilon } ^{2} &amp; \\beta _{1} \\sigma _{x_{1}} ^{2} &amp; \\beta _{2} \\sigma _{x_{2}} ^{2} \\\\ \\sigma _{x_{1}} ^{2} \\beta _{1} &amp; \\sigma _{x_{1}} ^{2} &amp; 0 \\\\ \\sigma _{x_{2}} ^{2} \\beta _{2} &amp; 0 &amp; \\sigma _{x_{2}} ^{2} \\end{array} \\right)\\end{equation*}\\] The mean expectations can be symbolically derived using the ramR::v_sym() function. ramR::v_sym(A, u) ## {{beta[0]+beta[1]*mu[x1]+beta[2]*mu[x2]}, ## { mu[x1]}, ## { mu[x2]}, ## { 0}} \\[\\begin{equation*}\\mathbf{v} =\\left( \\begin{array}{c} \\beta _{0} + \\beta _{1} \\mu _{x_{1}} + \\beta _{2} \\mu _{x_{2}} \\\\ \\mu _{x_{1}} \\\\ \\mu _{x_{2}} \\\\ 0 \\end{array} \\right)\\end{equation*}\\] The mean expectations for the observed variables can be symbolically derived using the ramR::g_sym() function. ramR::g_sym(A, u, filter) ## {{beta[0]+beta[1]*mu[x1]+beta[2]*mu[x2]}, ## { mu[x1]}, ## { mu[x2]}} \\[\\begin{equation*}\\mathbf{g} =\\left( \\begin{array}{c} \\beta _{0} + \\beta _{1} \\mu _{x_{1}} + \\beta _{2} \\mu _{x_{2}} \\\\ \\mu _{x_{1}} \\\\ \\mu _{x_{2}} \\end{array} \\right)\\end{equation*}\\] 5.2 Numerical Example Let df be a random sample with the following parameters Parameter \\(x = 0\\) \\(x = 1\\) \\(x = 2\\) Sample Size 500 500 500 \\(\\mu\\) 0 2 1 \\(\\sigma^2\\) 1 1 1 Parameter Description Estimate \\(\\beta_0\\) Mean of \\(x = 0\\). 0 \\(\\beta_1\\) Mean of \\(x = 1\\) minus \\(x = 0\\). 2 \\(\\beta_2\\) Mean of \\(x = 2\\) minus \\(x = 0\\). 1 head(df) ## y x ## 1 -0.6013830 0 ## 2 -0.1358161 0 ## 3 -0.9872728 0 ## 4 0.8319250 0 ## 5 -0.7950595 0 ## 6 0.3404646 0 summary(df) ## y x ## Min. :-2.61364 0:500 ## 1st Qu.: 0.08094 1:500 ## Median : 1.02617 2:500 ## Mean : 1.00814 ## 3rd Qu.: 1.90112 ## Max. : 5.47091 5.2.1 One-Way Analysis of Variance Make sure that \\(x\\) is of class factor for lm and aov to treat it as a categorical variable. str(df) ## &#39;data.frame&#39;: 1500 obs. of 2 variables: ## $ y: num -0.601 -0.136 -0.987 0.832 -0.795 ... ## $ x: Factor w/ 3 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;: 1 1 1 1 1 1 1 1 1 1 ... summary(aov(y ~ x, data = df)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x 2 983.8 491.9 471.4 &lt;2e-16 *** ## Residuals 1497 1562.2 1.0 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 5.2.2 Linear Regression summary(lm(y ~ x, data = df)) ## ## Call: ## lm(formula = y ~ x, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.1792 -0.6469 0.0021 0.6751 3.5538 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.03083 0.04569 0.675 0.5 ## x1 1.98309 0.06461 30.694 &lt;2e-16 *** ## x2 0.94884 0.06461 14.686 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.022 on 1497 degrees of freedom ## Multiple R-squared: 0.3864, Adjusted R-squared: 0.3856 ## F-statistic: 471.4 on 2 and 1497 DF, p-value: &lt; 2.2e-16 5.2.3 Structural Equation Modeling We have to dummy code the data set first before fitting the model. The model.matrix function which is used to create a design matrix can be used to dummy code x. Make sure that x is a factor. The first column of the design matrix is a matrix of ones. Since we do not need this column, we can replace this column with the values of y. Make sure to name rename the first column as lavaan relies on the column names. df_dummy &lt;- model.matrix(y ~ x, data = df) df_dummy[, 1] &lt;- df$y colnames(df_dummy)[1] &lt;- &quot;y&quot; head(df_dummy) ## y x1 x2 ## 1 -0.6013830 0 0 ## 2 -0.1358161 0 0 ## 3 -0.9872728 0 0 ## 4 0.8319250 0 0 ## 5 -0.7950595 0 0 ## 6 0.3404646 0 0 5.2.3.1 lavaan (Rosseel 2012) model &lt;- &quot; y ~ x1 + x2 &quot; fit &lt;- lavaan::sem( model, data = df_dummy, meanstructure = TRUE, fixed.x = FALSE ) lavaan::summary(fit) ## lavaan 0.6-7 ended normally after 22 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 9 ## ## Number of observations 1500 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## y ~ ## x1 1.983 0.065 30.725 0.000 ## x2 0.949 0.065 14.701 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## x1 ~~ ## x2 -0.111 0.006 -17.321 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .y 0.031 0.046 0.676 0.499 ## x1 0.333 0.012 27.386 0.000 ## x2 0.333 0.012 27.386 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .y 1.041 0.038 27.386 0.000 ## x1 0.222 0.008 27.386 0.000 ## x2 0.222 0.008 27.386 0.000 5.2.3.2 OpenMx (Boker et al. 2020) RAM matrices can be used to specify models in OpenMx. Note, however, that the u vector in the RAM notation is M in the OpenMx notation. mxData &lt;- OpenMx::mxData( observed = df_dummy, type = &quot;raw&quot; ) mxA &lt;- OpenMx::mxMatrix( type = &quot;Full&quot;, nrow = 4, ncol = 4, free = c( F, T, T, F, F, F, F, F, F, F, F, F, F, F, F, F ), values = c( 0, 0.20, 0.20, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ), labels = c( NA, &quot;beta1&quot;, &quot;beta2&quot;, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA ), byrow = TRUE, name = &quot;mxA&quot; ) mxS &lt;- OpenMx::mxMatrix( type = &quot;Symm&quot;, nrow = 4, ncol = 4, free = c( F, F, F, F, F, T, F, F, F, F, T, F, F, F, F, T ), values = c( 0, 0, 0, 0, 0, 0.20, 0, 0, 0, 0, 0.20, 0, 0, 0, 0, 0.20 ), labels = c( NA, NA, NA, NA, NA, &quot;sigma2x1&quot;, NA, NA, NA, NA, &quot;sigma2x2&quot;, NA, NA, NA, NA, &quot;sigma2e&quot; ), byrow = TRUE, name = &quot;mxS&quot; ) mxM &lt;- OpenMx::mxMatrix( type = &quot;Full&quot;, nrow = 1, ncol = 4, free = c( T, T, T, F ), values = c( 0.20, 0.20, 0.20, 0 ), labels = c( &quot;beta0&quot;, &quot;mux1&quot;, &quot;mux2&quot;, NA ), byrow = TRUE, name = &quot;mxM&quot; ) mxF &lt;- OpenMx::mxMatrix( type = &quot;Full&quot;, nrow = 3, ncol = 4, free = FALSE, values = c( 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0 ), byrow = TRUE, name = &quot;mxF&quot; ) expRAM &lt;- OpenMx::mxExpectationRAM( A = &quot;mxA&quot;, S = &quot;mxS&quot;, F = &quot;mxF&quot;, M = &quot;mxM&quot;, dimnames = c( &quot;y&quot;, &quot;x1&quot;, &quot;x2&quot;, &quot;e&quot; ) ) objML &lt;- OpenMx::mxFitFunctionML() mxMod &lt;- OpenMx::mxModel( name = &quot;One Way Analysis of Variance&quot;, data = mxData, matrices = list( mxA, mxS, mxF, mxM ), expectation = expRAM, fitfunction = objML ) fit &lt;- OpenMx::mxRun(mxMod) ## Running One Way Analysis of Variance with 8 parameters summary(fit) ## Summary of One Way Analysis of Variance ## ## free parameters: ## name matrix row col Estimate Std.Error A ## 1 beta1 mxA 1 2 1.98308662 0.064543779 ## 2 beta2 mxA 1 3 0.94883814 0.064543143 ## 3 sigma2x1 mxS 2 2 0.22222230 0.008114416 ## 4 sigma2x2 mxS 3 3 0.22222238 0.008114420 ## 5 sigma2e mxS 4 4 1.04147460 0.038029458 ## 6 beta0 mxM 1 y 0.03083127 0.045639092 ## 7 mux1 mxM 1 x1 0.33333343 0.012171613 ## 8 mux2 mxM 1 x2 0.33333344 0.012171612 ## ## Model Statistics: ## | Parameters | Degrees of Freedom | Fit (-2lnL units) ## Model: 8 4492 8319.17 ## Saturated: 9 4491 NA ## Independence: 6 4494 NA ## Number of observations/statistics: 1500/4500 ## ## Information Criteria: ## | df Penalty | Parameters Penalty | Sample-Size Adjusted ## AIC: -664.8302 8335.170 8335.266 ## BIC: -24531.8162 8377.676 8352.262 ## To get additional fit indices, see help(mxRefModels) ## timestamp: 2021-01-23 23:50:38 ## Wall clock time: 0.03214574 secs ## optimizer: SLSQP ## OpenMx version number: 2.18.1 ## Need help? See help(mxSummary) 5.2.4 Using the ramR Package A ## y x1 x2 e ## y 0 2.008444 0.9885797 1 ## x1 0 0.000000 0.0000000 0 ## x2 0 0.000000 0.0000000 0 ## e 0 0.000000 0.0000000 0 S ## y x1 x2 e ## y 0 0.0000000 0.0000000 0.0000000 ## x1 0 0.2223705 0.0000000 0.0000000 ## x2 0 0.0000000 0.2223705 0.0000000 ## e 0 0.0000000 0.0000000 0.9823083 u ## u ## y 0.3333333 ## x1 0.3333333 ## x2 0.3333333 ## e 0.0000000 filter ## y x1 x2 e ## y 1 0 0 0 ## x1 0 1 0 0 ## x2 0 0 1 0 The covariance expectations can be numerically derived using the ramR::C_num() function. ramR::C_num(A, S) ## y x1 x2 e ## y 2.0966368 0.4466185 0.2198309 0.9823083 ## x1 0.4466185 0.2223705 0.0000000 0.0000000 ## x2 0.2198309 0.0000000 0.2223705 0.0000000 ## e 0.9823083 0.0000000 0.0000000 0.9823083 The covariance expectations for the observed variables can be numerically derived using the ramR::M_num() function. ramR::M_num(A, S, filter) ## y x1 x2 ## y 2.0966368 0.4466185 0.2198309 ## x1 0.4466185 0.2223705 0.0000000 ## x2 0.2198309 0.0000000 0.2223705 The mean expectations can be numerically derived using the ramR::v_num() function. ramR::v_num(A, u) ## v ## y 1.3323411 ## x1 0.3333333 ## x2 0.3333333 ## e 0.0000000 The mean expectations for the observed variables can be numerically derived using the ramR::v_num() function. ramR::g_num(A, u, filter) ## g ## y 1.3323411 ## x1 0.3333333 ## x2 0.3333333 5.3 Equations to RAM The ramR package has a utility function to convert structural equations to RAM notation. One-way analysis of variance with three levels can be expressed in the following equations eq &lt;- &quot; # VARIABLE1 OPERATION VARIABLE2 LABEL e by y 1 y on x1 beta1 y on x2 beta2 e with e sigma[varepsilon]^2 x1 with x1 sigma[x1]^2 x2 with x2 sigma[x2]^2 y on 1 beta0 x1 on 1 mu[x1] x2 on 1 mu[x2] &quot; Figure 5.2: One-Way Analysis of Variance’s Structural Equations The error term is treated as a latent variable and defined with the operation by. Its value is constrained to \\(1\\). The regression of \\(y\\) on \\(x_1\\) and \\(x_2\\) is defined by operation on. The coefficients are labeled as beta[1] and beta[2] respectively. The variance of \\(x_1\\), \\(x_2\\) and the error variance are defined using the operation with. These are labeled sigma[x1]^2, sigma[x2]^2, and sigma[varepsilon]^2 respectively. The intercept and the mean of \\(x_1\\) and \\(x_2\\) are defined using the operation on 1. These are labeled beta[0], mu[x1], and mu[x2] respectively. The ramR::eq2ram converts the equations to RAM notation. ramR::eq2ram(eq) ## $eq ## var1 op var2 label ## 1 e by y 1 ## 2 y on x1 beta1 ## 3 y on x2 beta2 ## 4 e with e sigma[varepsilon]^2 ## 5 x1 with x1 sigma[x1]^2 ## 6 x2 with x2 sigma[x2]^2 ## 7 y on 1 beta0 ## 8 x1 on 1 mu[x1] ## 9 x2 on 1 mu[x2] ## ## $variables ## [1] &quot;y&quot; &quot;x1&quot; &quot;x2&quot; &quot;e&quot; ## ## $A ## y x1 x2 e ## y &quot;0&quot; &quot;beta1&quot; &quot;beta2&quot; &quot;1&quot; ## x1 &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## x2 &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## e &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## ## $S ## y x1 x2 e ## y &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## x1 &quot;0&quot; &quot;sigma[x1]^2&quot; &quot;0&quot; &quot;0&quot; ## x2 &quot;0&quot; &quot;0&quot; &quot;sigma[x2]^2&quot; &quot;0&quot; ## e &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;sigma[varepsilon]^2&quot; ## ## $filter ## y x1 x2 e ## y 1 0 0 0 ## x1 0 1 0 0 ## x2 0 0 1 0 ## ## $u ## u ## y &quot;beta0&quot; ## x1 &quot;mu[x1]&quot; ## x2 &quot;mu[x2]&quot; ## e &quot;0&quot; 5.4 Equations to Expectations The ramR package has a utility function to convert structural equations to expectations both symbolically and numerically. eq &lt;- &quot; # VARIABLE1 OPERATION VARIABLE2 LABEL e by y 1 y on x1 beta1 y on x2 beta2 e with e sigma[varepsilon]^2 x1 with x1 sigma[x1]^2 x2 with x2 sigma[x2]^2 y on 1 beta0 x1 on 1 mu[x1] x2 on 1 mu[x2] &quot; ramR::eq2exp_sym(eq) ## $variables ## [1] &quot;y&quot; &quot;x1&quot; &quot;x2&quot; &quot;e&quot; ## ## $A ## {{ 0, beta1, beta2, 1}, ## { 0, 0, 0, 0}, ## { 0, 0, 0, 0}, ## { 0, 0, 0, 0}} ## ## $S ## {{ 0, 0, 0, 0}, ## { 0, sigma[x1]^2, 0, 0}, ## { 0, 0, sigma[x2]^2, 0}, ## { 0, 0, 0, sigma[varepsilon]^2}} ## ## $u ## {{ beta0}, ## {mu[x1]}, ## {mu[x2]}, ## { 0}} ## ## $filter ## {{1, 0, 0, 0}, ## {0, 1, 0, 0}, ## {0, 0, 1, 0}} ## ## $v ## {{beta0+beta1*mu[x1]+beta2*mu[x2]}, ## { mu[x1]}, ## { mu[x2]}, ## { 0}} ## ## $g ## {{beta0+beta1*mu[x1]+beta2*mu[x2]}, ## { mu[x1]}, ## { mu[x2]}} ## ## $C ## {{sigma[x1]^2*beta1^2+sigma[x2]^2*beta2^2+sigma[varepsilon]^2, beta1*sigma[x1]^2, beta2*sigma[x2]^2, sigma[varepsilon]^2}, ## { sigma[x1]^2*beta1, sigma[x1]^2, 0, 0}, ## { sigma[x2]^2*beta2, 0, sigma[x2]^2, 0}, ## { sigma[varepsilon]^2, 0, 0, sigma[varepsilon]^2}} ## ## $M ## {{sigma[x1]^2*beta1^2+sigma[x2]^2*beta2^2+sigma[varepsilon]^2, beta1*sigma[x1]^2, beta2*sigma[x2]^2}, ## { sigma[x1]^2*beta1, sigma[x1]^2, 0}, ## { sigma[x2]^2*beta2, 0, sigma[x2]^2}} eq &lt;- &quot; # VARIABLE1 OPERATION VARIABLE2 LABEL e by y 1 y on x1 2 y on x2 1 e with e 1 x1 with x1 0.22222222222 x2 with x2 0.22222222222 y on 1 0 x1 on 1 0.33333333333 x2 on 1 0.33333333333 &quot; ramR::eq2exp_num(eq) ## $variables ## [1] &quot;y&quot; &quot;x1&quot; &quot;x2&quot; &quot;e&quot; ## ## $A ## y x1 x2 e ## y 0 2 1 1 ## x1 0 0 0 0 ## x2 0 0 0 0 ## e 0 0 0 0 ## ## $S ## y x1 x2 e ## y 0 0.0000000 0.0000000 0 ## x1 0 0.2222222 0.0000000 0 ## x2 0 0.0000000 0.2222222 0 ## e 0 0.0000000 0.0000000 1 ## ## $u ## u ## y 0.0000000 ## x1 0.3333333 ## x2 0.3333333 ## e 0.0000000 ## ## $filter ## y x1 x2 e ## y 1 0 0 0 ## x1 0 1 0 0 ## x2 0 0 1 0 ## ## $v ## v ## y 1.0000000 ## x1 0.3333333 ## x2 0.3333333 ## e 0.0000000 ## ## $g ## g ## y 1.0000000 ## x1 0.3333333 ## x2 0.3333333 ## ## $C ## y x1 x2 e ## y 2.1111111 0.4444444 0.2222222 1 ## x1 0.4444444 0.2222222 0.0000000 0 ## x2 0.2222222 0.0000000 0.2222222 0 ## e 1.0000000 0.0000000 0.0000000 1 ## ## $M ## y x1 x2 ## y 2.1111111 0.4444444 0.2222222 ## x1 0.4444444 0.2222222 0.0000000 ## x2 0.2222222 0.0000000 0.2222222 "],["ram-two-reg.html", "Chapter 6 Two-Variable Regression Model", " Chapter 6 Two-Variable Regression Model Figure 6.1: Two-Variable Regression Model "],["ram-k-reg.html", "Chapter 7 \\(k\\)-Variable Regression Model", " Chapter 7 \\(k\\)-Variable Regression Model Figure 7.1: \\(k\\)-Variable Regression Model "],["ram-med-simple.html", "Chapter 8 The Simple Mediation Model", " Chapter 8 The Simple Mediation Model Figure 8.1: The Simple Mediation Model "],["ram-med-simple-std.html", "Chapter 9 The Standardized Simple Mediation Model", " Chapter 9 The Standardized Simple Mediation Model Figure 9.1: The Standardized Simple Mediation Model "],["references.html", "References", " References Boker, Steven M., and John J. McArdle. 2005. “Path Analysis and Path Diagrams.” In Encyclopedia of Statistics in Behavioral Science, edited by Brian S. Everitt and David C. Howell, 1529–31. Chichester, UK: John Wiley &amp; Sons, Ltd. https://doi.org/10.1002/0470013192.bsa471. Boker, Steven M., Michael C. Neale, Hermine H. Maes, Michael J. Wilde, Michael Spiegel, Timothy R. Brick, Ryne Estabrook, et al. 2020. OpenMx 2.18.1 User Guide. McArdle, John J. 2005. “The Development of the RAM Rules for Latent Variable Structural Equation Modeling.” In Contemporary Psychometrics: A Festschrift for Roderick P. McDonald, edited by Albert Maydeu-Olivares and John J. McArdle, 225–73. Multivariate Applications Book Series. Mahwah, NJ: Lawrence Erlbaum Associates. McArdle, John J., and Roderick P. McDonald. 1984. “Some Algebraic Properties of the Reticular Action Model for Moment Structures.” British Journal of Mathematical and Statistical Psychology 37 (2): 234–51. https://doi.org/10.1111/j.2044-8317.1984.tb00802.x. Pesigan, Ivan Jacob Agaloos. 2021. ramR: Reticular Action Model (RAM) Notation. https://github.com/jeksterslab/ramR. R Core Team. 2020. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/. Rosseel, Yves. 2012. “lavaan: An R Package for Structural Equation Modeling.” Journal of Statistical Software 48 (2): 1–36. http://www.jstatsoft.org/v48/i02/. "]]
