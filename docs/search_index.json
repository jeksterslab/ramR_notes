[["index.html", "Reticular Action Model (RAM) Notation Notes Chapter 1 Description", " Reticular Action Model (RAM) Notation Notes Ivan Jacob Agaloos Pesigan 2021-02-14 Chapter 1 Description This is a collection of my notes on the Reticular Action Model (RAM) notation that accompanies the ramR package (Pesigan 2021) in the R statistical environment (R Core Team 2020). You can install the released version of ramR from GitHub with: remotes::install_github(&quot;jeksterslab/ramR&quot;) These notes are based on the following resources: Boker and McArdle (2005) McArdle and McDonald (1984) McArdle (2005) See GitHub Pages for the html deployment. "],["ram-matrix-notation.html", "Chapter 2 Reticular Action Model (RAM) Matrix Notation 2.1 Full Model 2.2 Given vs. Hidden Variables", " Chapter 2 Reticular Action Model (RAM) Matrix Notation 2.1 Full Model Definition 2.1 \\[\\begin{equation} \\mathbf{v} = \\mathbf{A} \\mathbf{v} + \\mathbf{u} \\end{equation}\\] where \\(\\mathbf{v}\\) and \\(\\mathbf{u}\\) are \\(t \\times 1\\) vectors of random variables \\(\\mathbf{A}\\) is a \\(t \\times t\\) matrix of directed or asymmetric relationship from column variable \\(v_j\\) to row variable \\(v_i\\) \\(\\mathbf{A}\\) represent the regression of each of the \\(t\\) variables \\(\\mathbf{v}\\) on the other \\(t - 1\\) variables diagonal \\(a_{i,i}\\) is zero \\(u_i\\) represent the residual of \\(v_i\\) if all regression coefficients on other variables are zero (i.e., \\(i^{\\mathrm{th}}\\) row of \\(\\mathbf{A}\\) consists of zeros), then the variable \\(v_i\\) is considered the same as its own residual \\(u_i\\) Definition 2.2 \\[\\begin{equation} \\mathbf{S} = \\mathbb{E} \\left\\{ \\mathbf{u} \\mathbf{u}^{\\prime} \\right\\} , \\end{equation}\\] where \\(\\mathbf{S}\\) is a \\(t \\times t\\) matrix of undirected or symmetric relationship the notation \\(\\boldsymbol{\\Omega}\\) is used in other sources for \\(\\mathbf{S}\\) \\(\\mathbb{E}\\) is the expectation operator Definition 2.3 \\[\\begin{equation} \\mathbf{C} = \\mathbb{E} \\left\\{ \\mathbf{v} \\mathbf{v}^{\\prime} \\right\\} , \\end{equation}\\] where \\(\\mathbf{C}\\) is a \\(t \\times t\\) variance-covariance matrix the notation \\(\\boldsymbol{\\Sigma}\\) is used in other sources for \\(\\mathbf{C}\\) Definition 2.4 \\[\\begin{equation*} \\mathbf{v} = \\mathbf{A} \\mathbf{v} + \\mathbf{u} \\end{equation*}\\] can be rewritten as \\[\\begin{equation} \\begin{split} \\mathbf{v} - \\mathbf{A} \\mathbf{v} &amp;= \\mathbf{u} \\\\ \\mathbf{u} &amp;= \\mathbf{v} - \\mathbf{A} \\mathbf{v} \\\\ \\mathbf{u} &amp;= \\left( \\mathbf{I} - \\mathbf{A} \\right) \\mathbf{v} \\end{split} \\end{equation}\\] assuming that \\(\\left( \\mathbf{I} - \\mathbf{A} \\right)\\) is non-singular, \\[\\begin{equation} \\mathbf{E} = \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\end{equation}\\] then \\[\\begin{equation} \\begin{split} \\mathbf{v} &amp;= \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\mathbf{u} \\\\ &amp;= \\mathbf{E} \\mathbf{u} . \\end{split} \\end{equation}\\] Using the definitions above, \\(\\mathbf{S}\\) and \\(\\mathbf{C}\\) are given by \\[\\begin{equation} \\begin{split} \\mathbf{S} &amp;= \\left( \\mathbf{I} - \\mathbf{A} \\right) \\mathbf{C} \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\\\ &amp;= \\mathbf{E}^{-1} \\mathbf{C} \\left( \\mathbf{E}^{-1} \\right)^{\\mathsf{T}} \\end{split} \\end{equation}\\] \\[\\begin{equation} \\begin{split} \\mathbf{C} &amp;= \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\mathbf{S} \\left[ \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\right]^{\\mathsf{T}} \\\\ &amp;= \\mathbf{E} \\mathbf{S} \\mathbf{E}^{\\mathsf{T}} \\end{split} \\end{equation}\\] 2.2 Given vs. Hidden Variables Definition 2.5 \\[\\begin{equation} \\mathbf{v} = \\begin{bmatrix} \\mathbf{g}_{p \\times 1} \\\\ \\mathbf{h}_{q \\times 1} \\\\ \\end{bmatrix} \\end{equation}\\] \\[\\begin{equation} t = p + q \\end{equation}\\] \\(\\mathbf{g}\\) may be considered observed, manifest or given variables \\(\\mathbf{h}\\) may be considered unobserved, latent, or hidden variables Definition 2.6 \\[\\begin{equation} \\mathbf{F} = \\begin{bmatrix} \\mathbf{I}_{p \\times p} : \\mathbf{0}_{p \\times q} \\end{bmatrix} \\end{equation}\\] the \\(\\mathbf{F}\\) matrix acts as a filter to select the manifest variables out of the full set of manifest and latent variables \\[\\begin{equation} \\mathbf{g} = \\mathbf{F} \\mathbf{v} \\end{equation}\\] \\[\\begin{equation} \\begin{split} \\mathbf{g} &amp;= \\mathbf{F} \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\mathbf{u} \\\\ &amp;= \\mathbf{F} \\mathbf{E} \\mathbf{u} \\end{split} \\end{equation}\\] Definition 2.7 \\[\\begin{equation} \\mathbf{M} = \\mathbb{E} \\left\\{ \\mathbf{g} \\mathbf{g}^{\\mathsf{T}} \\right\\} \\end{equation}\\] \\[\\begin{equation} \\begin{split} \\mathbf{M} &amp;= \\mathbf{F} \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\mathbf{S} \\left[ \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\right]^{\\mathsf{T}} \\mathbf{F}^{\\mathsf{T}} \\\\ &amp;= \\mathbf{F} \\mathbf{E} \\mathbf{S} \\mathbf{E}^{\\mathsf{T}} \\mathbf{F}^{\\mathsf{T}} \\\\ &amp;= \\mathbf{F} \\mathbf{C} \\mathbf{F}^{\\mathsf{T}} \\end{split} \\end{equation}\\] when components of \\(\\mathbf{v}\\) are permuted, the columns of \\(\\mathbf{F}\\) can be correspondingly permuted the rows and columns of \\(\\mathbf{C}\\) that are filtered out by \\(\\mathbf{F}\\) contain useful information about the latent variable structure. The equations above completely define RAM. "],["ram-diagram.html", "Chapter 3 Reticular Action Model (RAM) Path Diagram", " Chapter 3 Reticular Action Model (RAM) Path Diagram Figure 3.1: Path Diagram Elements Figure 3.2: Two-Variable Regression Model Figure 3.3: \\(k\\)-Variable Regression Model Figure 3.4: Two-Factor Confirmatory Factor Analysis Model Figure 3.5: Two-Factor Confirmatory Factor Analysis Model with Mean Structure Figure 3.6: Path Model with Latent Variables "],["ram-t.html", "Chapter 4 Student’s \\(t\\)-test 4.1 Symbolic 4.2 Numerical Example 4.3 Equations to RAM 4.4 Equations to Expectations", " Chapter 4 Student’s \\(t\\)-test In this section, the Student’s \\(t\\)-test is presented as a structural equation model using the RAM notation. Let \\(y\\) be a continuous dependent variable, \\(x\\) be a dichotomous independent variable \\(\\left( x = \\{0, 1\\} \\right)\\), and \\(\\varepsilon\\) be the stochastic error term with mean 0 and constant variance of \\(\\sigma_{\\varepsilon}^{2}\\) across the values of \\(x\\). The associations of the variables are given by \\[\\begin{equation*} y = \\alpha + \\beta x + \\varepsilon \\end{equation*}\\] where \\(\\alpha\\) is the expected value of \\(y\\) when \\(x = 0\\) \\(\\beta\\) is the unit change in \\(y\\) for unit change in \\(x\\) \\(\\alpha + \\beta\\) is the expected value of \\(y\\) when \\(x = 1\\) Figure 4.1: Student’s \\(t\\)-test 4.1 Symbolic Let \\(\\left\\{ y, x, \\varepsilon \\right\\}\\) be the variables of interest. \\[\\begin{align*}\\mathbf{A} &amp;=\\left( \\begin{array}{ccc} 0 &amp; \\beta &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{S} &amp;=\\left( \\begin{array}{ccc} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma _{x} ^{2} &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma _{\\varepsilon } ^{2} \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{C} &amp;=\\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\mathbf{S} \\left[ \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\right]^{\\mathsf{T}} \\\\\\\\ &amp;=\\mathbf{E} \\mathbf{S} \\mathbf{E}^{\\mathsf{T}} \\\\\\\\ &amp;=\\left( \\begin{array}{ccc} 1 &amp; \\beta &amp; 1 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array} \\right)\\left( \\begin{array}{ccc} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma _{x} ^{2} &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma _{\\varepsilon } ^{2} \\end{array} \\right)\\left( \\begin{array}{ccc} 1 &amp; \\beta &amp; 1 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array} \\right)^{\\mathsf{T}}\\\\ &amp;=\\left( \\begin{array}{ccc} \\sigma _{x} ^{2} \\beta ^{2} + \\sigma _{\\varepsilon } ^{2} &amp; \\beta \\sigma _{x} ^{2} &amp; \\sigma _{\\varepsilon } ^{2} \\\\ \\sigma _{x} ^{2} \\beta &amp; \\sigma _{x} ^{2} &amp; 0 \\\\ \\sigma _{\\varepsilon } ^{2} &amp; 0 &amp; \\sigma _{\\varepsilon } ^{2} \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{F} &amp;=\\left( \\begin{array}{ccc} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{M} &amp;=\\mathbf{F} \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\mathbf{S} \\left[ \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\right]^{\\mathsf{T}} \\mathbf{F}^{\\mathsf{T}}\\\\ &amp;=\\mathbf{F} \\mathbf{E} \\mathbf{S} \\mathbf{E}^{\\mathsf{T}} \\mathbf{F}^{\\mathsf{T}} \\\\\\\\ &amp;=\\mathbf{F} \\mathbf{C} \\mathbf{F}^{\\mathsf{T}} \\\\\\\\ &amp;=\\left( \\begin{array}{ccc} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\end{array} \\right)\\left( \\begin{array}{ccc} \\sigma _{x} ^{2} \\beta ^{2} + \\sigma _{\\varepsilon } ^{2} &amp; \\beta \\sigma _{x} ^{2} &amp; \\sigma _{\\varepsilon } ^{2} \\\\ \\sigma _{x} ^{2} \\beta &amp; \\sigma _{x} ^{2} &amp; 0 \\\\ \\sigma _{\\varepsilon } ^{2} &amp; 0 &amp; \\sigma _{\\varepsilon } ^{2} \\end{array} \\right)\\left( \\begin{array}{ccc} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\end{array} \\right)^{\\mathsf{T}} \\\\\\\\ &amp;=\\left( \\begin{array}{cc} \\sigma _{x} ^{2} \\beta ^{2} + \\sigma _{\\varepsilon } ^{2} &amp; \\beta \\sigma _{x} ^{2} \\\\ \\sigma _{x} ^{2} \\beta &amp; \\sigma _{x} ^{2} \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{v} &amp;=\\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\mathbf{u}\\\\ &amp;=\\left[\\left( \\begin{array}{ccc} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array} \\right)-\\left( \\begin{array}{ccc} 0 &amp; \\beta &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{array} \\right)\\right]^{\\mathsf{-1}}\\left( \\begin{array}{c} \\alpha \\\\ \\mu _{x} \\\\ 0 \\end{array} \\right)\\\\ &amp;=\\left( \\begin{array}{c} \\alpha + \\beta \\mu _{x} \\\\ \\mu _{x} \\\\ 0 \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{u} &amp;=\\left( \\mathbf{I} - \\mathbf{A} \\right) \\mathbf{v}\\\\ &amp;=\\left[\\left( \\begin{array}{ccc} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array} \\right)-\\left( \\begin{array}{ccc} 0 &amp; \\beta &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{array} \\right)\\right]\\left( \\begin{array}{c} \\alpha + \\beta \\mu _{x} \\\\ \\mu _{x} \\\\ 0 \\end{array} \\right)\\\\ &amp;=\\left( \\begin{array}{c} \\alpha + \\beta \\mu _{x} \\\\ \\mu _{x} \\\\ 0 \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{g} &amp;=\\mathbf{F} \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\mathbf{u}\\\\ &amp;=\\left[\\left( \\begin{array}{ccc} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array} \\right)-\\left( \\begin{array}{ccc} 0 &amp; \\beta &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{array} \\right)\\right]^{-1}\\left( \\begin{array}{c} \\alpha \\\\ \\mu _{x} \\\\ 0 \\end{array} \\right)\\\\ &amp;=\\left( \\begin{array}{c} \\alpha + \\beta \\mu _{x} \\\\ \\mu _{x} \\end{array} \\right)\\end{align*}\\] 4.1.1 Using the ramR Package A ## y x e ## y &quot;0&quot; &quot;beta&quot; &quot;1&quot; ## x &quot;0&quot; &quot;0&quot; &quot;0&quot; ## e &quot;0&quot; &quot;0&quot; &quot;0&quot; S ## y x e ## y &quot;0&quot; &quot;0&quot; &quot;0&quot; ## x &quot;0&quot; &quot;sigma[x]^2&quot; &quot;0&quot; ## e &quot;0&quot; &quot;0&quot; &quot;sigma[varepsilon]^2&quot; u ## u ## y &quot;alpha&quot; ## x &quot;mu[x]&quot; ## e &quot;0&quot; Filter ## y x e ## y 1 0 0 ## x 0 1 0 The covariance expectations can be symbolically derived using the ramR::C() function with A of class yac.symbol. ramR::C(Ryacas::ysym(A), S) ## {{sigma[x]^2*beta^2+sigma[varepsilon]^2, beta*sigma[x]^2, sigma[varepsilon]^2}, ## { sigma[x]^2*beta, sigma[x]^2, 0}, ## { sigma[varepsilon]^2, 0, sigma[varepsilon]^2}} \\[\\begin{equation*}\\mathbf{C} =\\left( \\begin{array}{ccc} \\sigma _{x} ^{2} \\beta ^{2} + \\sigma _{\\varepsilon } ^{2} &amp; \\beta \\sigma _{x} ^{2} &amp; \\sigma _{\\varepsilon } ^{2} \\\\ \\sigma _{x} ^{2} \\beta &amp; \\sigma _{x} ^{2} &amp; 0 \\\\ \\sigma _{\\varepsilon } ^{2} &amp; 0 &amp; \\sigma _{\\varepsilon } ^{2} \\end{array} \\right)\\end{equation*}\\] The covariance expectations for the observed variables can be symbolically derived using the ramR::M() function with A of class yac.symbol. ramR::M(Ryacas::ysym(A), S, Filter) ## {{sigma[x]^2*beta^2+sigma[varepsilon]^2, beta*sigma[x]^2}, ## { sigma[x]^2*beta, sigma[x]^2}} \\[\\begin{equation*}\\mathbf{M} =\\left( \\begin{array}{cc} \\sigma _{x} ^{2} \\beta ^{2} + \\sigma _{\\varepsilon } ^{2} &amp; \\beta \\sigma _{x} ^{2} \\\\ \\sigma _{x} ^{2} \\beta &amp; \\sigma _{x} ^{2} \\end{array} \\right)\\end{equation*}\\] The mean expectations can be symbolically derived using the ramR::v() function with A of class yac.symbol. ramR::v(Ryacas::ysym(A), u) ## {{alpha+beta*mu[x]}, ## { mu[x]}, ## { 0}} \\[\\begin{equation*}\\mathbf{v} =\\left( \\begin{array}{c} \\alpha + \\beta \\mu _{x} \\\\ \\mu _{x} \\\\ 0 \\end{array} \\right)\\end{equation*}\\] The mean expectations for the observed variables can be symbolically derived using the ramR::g() function with A of class yac.symbol. ramR::g(Ryacas::ysym(A), u, Filter) ## {{alpha+beta*mu[x]}, ## { mu[x]}} \\[\\begin{equation*}\\mathbf{g} =\\left( \\begin{array}{c} \\alpha + \\beta \\mu _{x} \\\\ \\mu _{x} \\end{array} \\right)\\end{equation*}\\] 4.2 Numerical Example Let df be a random sample from a population with the following parameters Parameter \\(x = 0\\) \\(x = 1\\) Sample Size 500 500 \\(\\mathbb{E} \\left( y \\mid x \\right)\\) 0 1 \\(\\mathrm{Var} \\left( y \\mid x \\right)\\) 1 1 Parameter Description Value \\(\\alpha\\) \\(\\mathbb{E} \\left( y \\mid x = 0 \\right)\\) 0 \\(\\beta\\) \\(\\mathbb{E} \\left( y \\mid x = 1 \\right) - \\mathbb{E} \\left( y \\mid x = 0 \\right)\\) 1 head(df) ## y x ## 1 1.3709584 0 ## 2 -0.5646982 0 ## 3 0.3631284 0 ## 4 0.6328626 0 ## 5 0.4042683 0 ## 6 -0.1061245 0 summary(df) ## y x ## Min. :-2.9931 Min. :0.0 ## 1st Qu.:-0.2770 1st Qu.:0.0 ## Median : 0.4503 Median :0.5 ## Mean : 0.4742 Mean :0.5 ## 3rd Qu.: 1.2492 3rd Qu.:1.0 ## Max. : 4.4953 Max. :1.0 4.2.1 \\(t\\)-test t.test(y ~ x, data = df) ## ## Welch Two Sample t-test ## ## data: y by x ## t = -15.897, df = 994.36, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.1329278 -0.8839594 ## sample estimates: ## mean in group 0 mean in group 1 ## -0.03004622 0.97839737 4.2.2 Linear Regression summary(lm(y ~ x, data = df)) ## ## Call: ## lm(formula = y ~ x, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.3501 -0.6517 0.0086 0.6858 3.5169 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.03005 0.04486 -0.67 0.503 ## x 1.00844 0.06344 15.90 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.003 on 998 degrees of freedom ## Multiple R-squared: 0.2021, Adjusted R-squared: 0.2013 ## F-statistic: 252.7 on 1 and 998 DF, p-value: &lt; 2.2e-16 4.2.3 Structural Equation Modeling 4.2.3.1 lavaan (Rosseel 2012) model &lt;- &quot; y ~ x &quot; fit &lt;- lavaan::sem( model, data = df, meanstructure = TRUE, fixed.x = FALSE ) lavaan::summary(fit) ## lavaan 0.6-7 ended normally after 12 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 5 ## ## Number of observations 1000 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## y ~ ## x 1.008 0.063 15.913 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .y -0.030 0.045 -0.671 0.503 ## x 0.500 0.016 31.623 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .y 1.004 0.045 22.361 0.000 ## x 0.250 0.011 22.361 0.000 4.2.3.2 OpenMx (Boker et al. 2020) RAM matrices can be used to specify models in OpenMx. Note, however, that the u vector in the RAM notation is M in the OpenMx notation. mxData &lt;- OpenMx::mxData( observed = df, type = &quot;raw&quot; ) mxA &lt;- OpenMx::mxMatrix( type = &quot;Full&quot;, nrow = 3, ncol = 3, free = c( F, T, F, F, F, F, F, F, F ), values = c( 0, 0.20, 1, 0, 0, 0, 0, 0, 0 ), labels = c( NA, &quot;beta&quot;, NA, NA, NA, NA, NA, NA, NA ), byrow = TRUE, name = &quot;mxA&quot; ) mxS &lt;- OpenMx::mxMatrix( type = &quot;Symm&quot;, nrow = 3, ncol = 3, free = c( F, F, F, F, T, F, F, F, T ), values = c( 0, 0, 0, 0, 0.20, 0, 0, 0, 0.20 ), labels = c( NA, NA, NA, NA, &quot;sigma2x&quot;, NA, NA, NA, &quot;sigma2e&quot; ), byrow = TRUE, name = &quot;mxS&quot; ) mxM &lt;- OpenMx::mxMatrix( type = &quot;Full&quot;, nrow = 1, ncol = 3, free = c( T, T, F ), values = c( 0.20, 0.20, 0 ), labels = c( &quot;alpha&quot;, &quot;mux&quot;, NA ), byrow = TRUE, name = &quot;mxM&quot; ) mxF &lt;- OpenMx::mxMatrix( type = &quot;Full&quot;, nrow = 2, ncol = 3, free = FALSE, values = c( 1, 0, 0, 0, 1, 0 ), byrow = TRUE, name = &quot;mxF&quot; ) expRAM &lt;- OpenMx::mxExpectationRAM( A = &quot;mxA&quot;, S = &quot;mxS&quot;, F = &quot;mxF&quot;, M = &quot;mxM&quot;, dimnames = c( &quot;y&quot;, &quot;x&quot;, &quot;e&quot; ) ) objML &lt;- OpenMx::mxFitFunctionML() mxMod &lt;- OpenMx::mxModel( name = &quot;Student&#39;s t test&quot;, data = mxData, matrices = list( mxA, mxS, mxF, mxM ), expectation = expRAM, fitfunction = objML ) fit &lt;- OpenMx::mxRun(mxMod) ## Running Student&#39;s t test with 5 parameters summary(fit) ## Summary of Student&#39;s t test ## ## free parameters: ## name matrix row col Estimate Std.Error A ## 1 beta mxA 1 2 1.00844356 0.06337369 ## 2 sigma2x mxS 2 2 0.25000000 0.01118034 ## 3 sigma2e mxS 3 3 1.00402596 0.04490152 ## 4 alpha mxM 1 y -0.03004621 0.04481202 ## 5 mux mxM 1 x 0.49999999 0.01581140 ## ## Model Statistics: ## | Parameters | Degrees of Freedom | Fit (-2lnL units) ## Model: 5 1995 4293.478 ## Saturated: 5 1995 NA ## Independence: 4 1996 NA ## Number of observations/statistics: 1000/2000 ## ## Information Criteria: ## | df Penalty | Parameters Penalty | Sample-Size Adjusted ## AIC: 303.4776 4303.478 4303.538 ## BIC: -9487.4941 4328.016 4312.136 ## CFI: NA ## TLI: 1 (also known as NNFI) ## RMSEA: 0 [95% CI (NA, NA)] ## Prob(RMSEA &lt;= 0.05): NA ## To get additional fit indices, see help(mxRefModels) ## timestamp: 2021-02-14 03:27:14 ## Wall clock time: 0.0543139 secs ## optimizer: SLSQP ## OpenMx version number: 2.18.1 ## Need help? See help(mxSummary) 4.2.4 Using the ramR Package A ## y x e ## y 0 1.008444 1 ## x 0 0.000000 0 ## e 0 0.000000 0 S ## y x e ## y 0 0.0000000 0.000000 ## x 0 0.2502503 0.000000 ## e 0 0.0000000 1.006038 u ## u ## y -0.03004622 ## x 0.50000000 ## e 0.00000000 Filter ## y x e ## y 1 0 0 ## x 0 1 0 The covariance expectations can be numerically derived using the ramR::C() function. ramR::C(A, S) ## y x e ## y 1.2605321 0.2523633 1.006038 ## x 0.2523633 0.2502503 0.000000 ## e 1.0060380 0.0000000 1.006038 The covariance expectations for the observed variables can be numerically derived using the ramR::M() function. ramR::M(A, S, Filter) ## y x ## y 1.2605321 0.2523633 ## x 0.2523633 0.2502503 The mean expectations can be numerically derived using the ramR::v() function. ramR::v(A, u) ## v ## y 0.4741756 ## x 0.5000000 ## e 0.0000000 The mean expectations for the observed variables can be numerically derived using the ramR::g() function. ramR::g(A, u, Filter) ## g ## y 0.4741756 ## x 0.5000000 4.3 Equations to RAM The ramR package has a utility function to convert structural equations to RAM notation. The Student’s \\(t\\)-test can be expressed in the following equations eq &lt;- &quot; # LHS OPERATION RHS LABEL e by y 1 y on x beta e with e sigma[varepsilon]^2 x with x sigma[x]^2 y on 1 alpha x on 1 mu[x] &quot; Figure 4.2: Student’s \\(t\\)-test’s Structural Equations The error term is treated as a latent variable and defined with the operation by. Its value is constrained to \\(1\\). The regression of \\(y\\) on \\(x\\) is defined by operation on. It is labeled as beta. The variance of \\(x\\) and the error variance are defined using the operation with. These are labeled sigma[x]^2 and sigma[varepsilon]^2 respectively. The intercept and the mean of \\(x\\) are defined using the operation on 1. These are labeled alpha and mu[x] respectively. The ramR::Eq2RAM converts the equations to RAM notation. ramR::Eq2RAM(eq) ## $par.table ## lhs op rhs par.label par.index ## 1 e by y 1 1 ## 2 y on x beta p1 ## 3 e with e sigma[varepsilon]^2 p2 ## 4 x with x sigma[x]^2 p3 ## 5 y on 1 alpha p4 ## 6 x on 1 mu[x] p5 ## ## $variables ## [1] &quot;y&quot; &quot;x&quot; &quot;e&quot; ## ## $g.variables ## [1] &quot;y&quot; &quot;x&quot; ## ## $h.variables ## [1] &quot;e&quot; ## ## $A ## y x e ## y &quot;0&quot; &quot;beta&quot; &quot;1&quot; ## x &quot;0&quot; &quot;0&quot; &quot;0&quot; ## e &quot;0&quot; &quot;0&quot; &quot;0&quot; ## ## $S ## y x e ## y &quot;0&quot; &quot;0&quot; &quot;0&quot; ## x &quot;0&quot; &quot;sigma[x]^2&quot; &quot;0&quot; ## e &quot;0&quot; &quot;0&quot; &quot;sigma[varepsilon]^2&quot; ## ## $u ## u ## y &quot;alpha&quot; ## x &quot;mu[x]&quot; ## e &quot;0&quot; ## ## $Filter ## y x e ## y 1 0 0 ## x 0 1 0 4.4 Equations to Expectations The ramR package has a utility function to convert structural equations to expectations both symbolically and numerically. eq &lt;- &quot; # LHS OPERATION RHS LABEL e by y 1 y on x beta e with e sigma[varepsilon]^2 x with x sigma[x]^2 y on 1 alpha x on 1 mu[x] &quot; ramR::Eq2Expectations(eq) ## $par.table ## lhs op rhs par.label par.index ## 1 e by y 1 1 ## 2 y on x beta p1 ## 3 e with e sigma[varepsilon]^2 p2 ## 4 x with x sigma[x]^2 p3 ## 5 y on 1 alpha p4 ## 6 x on 1 mu[x] p5 ## ## $variables ## [1] &quot;y&quot; &quot;x&quot; &quot;e&quot; ## ## $g.variables ## [1] &quot;y&quot; &quot;x&quot; ## ## $h.variables ## [1] &quot;e&quot; ## ## $A ## {{ 0, beta, 1}, ## { 0, 0, 0}, ## { 0, 0, 0}} ## ## $S ## {{ 0, 0, 0}, ## { 0, sigma[x]^2, 0}, ## { 0, 0, sigma[varepsilon]^2}} ## ## $u ## {{alpha}, ## {mu[x]}, ## { 0}} ## ## $Filter ## {{1, 0, 0}, ## {0, 1, 0}} ## ## $v ## {{alpha+beta*mu[x]}, ## { mu[x]}, ## { 0}} ## ## $g ## {{alpha+beta*mu[x]}, ## { mu[x]}} ## ## $C ## {{sigma[x]^2*beta^2+sigma[varepsilon]^2, beta*sigma[x]^2, sigma[varepsilon]^2}, ## { sigma[x]^2*beta, sigma[x]^2, 0}, ## { sigma[varepsilon]^2, 0, sigma[varepsilon]^2}} ## ## $M ## {{sigma[x]^2*beta^2+sigma[varepsilon]^2, beta*sigma[x]^2}, ## { sigma[x]^2*beta, sigma[x]^2}} eq &lt;- &quot; # LHS OPERATION RHS VALUE e by y 1.00 y on x 1.00 e with e 1.00 x with x 0.25 y on 1 0.00 x on 1 0.50 &quot; ramR::Eq2Expectations(eq) ## $par.table ## lhs op rhs par.label par.index ## 1 e by y 1.00 1.00 ## 2 y on x 1.00 1.00 ## 3 e with e 1.00 1.00 ## 4 x with x 0.25 0.25 ## 5 y on 1 0.00 0.00 ## 6 x on 1 0.50 0.50 ## ## $variables ## [1] &quot;y&quot; &quot;x&quot; &quot;e&quot; ## ## $g.variables ## [1] &quot;y&quot; &quot;x&quot; ## ## $h.variables ## [1] &quot;e&quot; ## ## $A ## y x e ## y 0 1 1 ## x 0 0 0 ## e 0 0 0 ## ## $S ## y x e ## y 0 0.00 0 ## x 0 0.25 0 ## e 0 0.00 1 ## ## $u ## u ## y 0.0 ## x 0.5 ## e 0.0 ## ## $Filter ## y x e ## y 1 0 0 ## x 0 1 0 ## ## $v ## v ## y 0.5 ## x 0.5 ## e 0.0 ## ## $g ## g ## y 0.5 ## x 0.5 ## ## $C ## y x e ## y 1.25 0.25 1 ## x 0.25 0.25 0 ## e 1.00 0.00 1 ## ## $M ## y x ## y 1.25 0.25 ## x 0.25 0.25 "],["ram-anova.html", "Chapter 5 One-Way Analysis of Variance 5.1 Symbolic 5.2 Numerical Example 5.3 Equations to RAM 5.4 Equations to Expectations", " Chapter 5 One-Way Analysis of Variance In this section, one-way analysis of variance is presented as a structural equation model using the RAM notation. Let \\(y\\) be a continuous dependent variable, \\(x\\) be a categorical independent variable with three levels \\(\\left( x = \\{0, 1, 2\\} \\right)\\). The dependent variable \\(x\\) can be dummy coded as \\(x\\) \\(x_1\\) \\(x_2\\) \\(x = 0\\) 0 0 \\(x = 1\\) 1 0 \\(x = 2\\) 0 1 \\(\\varepsilon\\) is the stochastic error term with mean 0 and constant variance of \\(\\sigma_{\\varepsilon}^{2}\\) across the values of the regressors. The associations of the variables are given by \\[\\begin{equation*} y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon \\end{equation*}\\] where \\(\\beta_0\\) is the expected value of \\(y\\) when \\(x = 0\\) \\(\\beta_1\\) is the unit change in \\(y\\) for unit change in \\(x_1\\) while \\(x_2\\) is constant \\(\\beta_2\\) is the unit change in \\(y\\) for unit change in \\(x_2\\) while \\(x_1\\) is constant \\(\\beta_0 + \\beta_1\\) is the expected value of \\(y\\) when \\(x = 1\\) \\(\\beta_0 + \\beta_2\\) is the expected value of \\(y\\) when \\(x = 2\\) Figure 5.1: One-Way Analysis of Variance 5.1 Symbolic Let \\(\\left\\{ y, x_1, x_2, \\varepsilon \\right\\}\\) be the variables of interest. \\[\\begin{align*}\\mathbf{A} &amp;=\\left( \\begin{array}{cccc} 0 &amp; \\beta _{1} &amp; \\beta _{2} &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{S} &amp;=\\left( \\begin{array}{cccc} 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma _{x _{1}} ^{2} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma _{x _{2}} ^{2} &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\sigma _{\\varepsilon } ^{2} \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{C} &amp;=\\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\mathbf{S} \\left[ \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\right]^{\\mathsf{T}} \\\\\\\\ &amp;=\\mathbf{E} \\mathbf{S} \\mathbf{E}^{\\mathsf{T}} \\\\\\\\ &amp;=\\left( \\begin{array}{cccc} 1 &amp; \\beta _{1} &amp; \\beta _{2} &amp; 1 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{array} \\right)\\left( \\begin{array}{cccc} 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma _{x _{1}} ^{2} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma _{x _{2}} ^{2} &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\sigma _{\\varepsilon } ^{2} \\end{array} \\right)\\left( \\begin{array}{cccc} 1 &amp; \\beta _{1} &amp; \\beta _{2} &amp; 1 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{array} \\right)^{\\mathsf{T}}\\\\ &amp;=\\left( \\begin{array}{cccc} \\sigma _{x _{1}} ^{2} \\beta _{1} ^{2} + \\sigma _{x _{2}} ^{2} \\beta _{2} ^{2} + \\sigma _{\\varepsilon } ^{2} &amp; \\beta _{1} \\sigma _{x _{1}} ^{2} &amp; \\beta _{2} \\sigma _{x _{2}} ^{2} &amp; \\sigma _{\\varepsilon } ^{2} \\\\ \\sigma _{x _{1}} ^{2} \\beta _{1} &amp; \\sigma _{x _{1}} ^{2} &amp; 0 &amp; 0 \\\\ \\sigma _{x _{2}} ^{2} \\beta _{2} &amp; 0 &amp; \\sigma _{x _{2}} ^{2} &amp; 0 \\\\ \\sigma _{\\varepsilon } ^{2} &amp; 0 &amp; 0 &amp; \\sigma _{\\varepsilon } ^{2} \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{F} &amp;=\\left( \\begin{array}{cccc} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{M} &amp;=\\mathbf{F} \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\mathbf{S} \\left[ \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\right]^{\\mathsf{T}} \\mathbf{F}^{\\mathsf{T}}\\\\ &amp;=\\mathbf{F} \\mathbf{E} \\mathbf{S} \\mathbf{E}^{\\mathsf{T}} \\mathbf{F}^{\\mathsf{T}} \\\\\\\\ &amp;=\\mathbf{F} \\mathbf{C} \\mathbf{F}^{\\mathsf{T}} \\\\\\\\ &amp;=\\left( \\begin{array}{cccc} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\end{array} \\right)\\left( \\begin{array}{cccc} \\sigma _{x _{1}} ^{2} \\beta _{1} ^{2} + \\sigma _{x _{2}} ^{2} \\beta _{2} ^{2} + \\sigma _{\\varepsilon } ^{2} &amp; \\beta _{1} \\sigma _{x _{1}} ^{2} &amp; \\beta _{2} \\sigma _{x _{2}} ^{2} &amp; \\sigma _{\\varepsilon } ^{2} \\\\ \\sigma _{x _{1}} ^{2} \\beta _{1} &amp; \\sigma _{x _{1}} ^{2} &amp; 0 &amp; 0 \\\\ \\sigma _{x _{2}} ^{2} \\beta _{2} &amp; 0 &amp; \\sigma _{x _{2}} ^{2} &amp; 0 \\\\ \\sigma _{\\varepsilon } ^{2} &amp; 0 &amp; 0 &amp; \\sigma _{\\varepsilon } ^{2} \\end{array} \\right)\\left( \\begin{array}{cccc} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\end{array} \\right)^{\\mathsf{T}} \\\\\\\\ &amp;=\\left( \\begin{array}{ccc} \\sigma _{x _{1}} ^{2} \\beta _{1} ^{2} + \\sigma _{x _{2}} ^{2} \\beta _{2} ^{2} + \\sigma _{\\varepsilon } ^{2} &amp; \\beta _{1} \\sigma _{x _{1}} ^{2} &amp; \\beta _{2} \\sigma _{x _{2}} ^{2} \\\\ \\sigma _{x _{1}} ^{2} \\beta _{1} &amp; \\sigma _{x _{1}} ^{2} &amp; 0 \\\\ \\sigma _{x _{2}} ^{2} \\beta _{2} &amp; 0 &amp; \\sigma _{x _{2}} ^{2} \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{v} &amp;=\\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\mathbf{u}\\\\ &amp;=\\left[\\left( \\begin{array}{cccc} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{array} \\right)-\\left( \\begin{array}{cccc} 0 &amp; \\beta _{1} &amp; \\beta _{2} &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{array} \\right)\\right]^{\\mathsf{-1}}\\left( \\begin{array}{c} \\beta _{0} \\\\ \\mu _{x _{1}} \\\\ \\mu _{x _{2}} \\\\ 0 \\end{array} \\right)\\\\ &amp;=\\left( \\begin{array}{c} \\beta _{0} + \\beta _{1} \\mu _{x _{1}} + \\beta _{2} \\mu _{x _{2}} \\\\ \\mu _{x _{1}} \\\\ \\mu _{x _{2}} \\\\ 0 \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{u} &amp;=\\left( \\mathbf{I} - \\mathbf{A} \\right) \\mathbf{v}\\\\ &amp;=\\left[\\left( \\begin{array}{cccc} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{array} \\right)-\\left( \\begin{array}{cccc} 0 &amp; \\beta _{1} &amp; \\beta _{2} &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{array} \\right)\\right]\\left( \\begin{array}{c} \\beta _{0} + \\beta _{1} \\mu _{x _{1}} + \\beta _{2} \\mu _{x _{2}} \\\\ \\mu _{x _{1}} \\\\ \\mu _{x _{2}} \\\\ 0 \\end{array} \\right)\\\\ &amp;=\\left( \\begin{array}{c} \\beta _{0} + \\beta _{1} \\mu _{x _{1}} + \\beta _{2} \\mu _{x _{2}} \\\\ \\mu _{x _{1}} \\\\ \\mu _{x _{2}} \\\\ 0 \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{g} &amp;=\\mathbf{F} \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\mathbf{u}\\\\ &amp;=\\left[\\left( \\begin{array}{cccc} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{array} \\right)-\\left( \\begin{array}{cccc} 0 &amp; \\beta _{1} &amp; \\beta _{2} &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{array} \\right)\\right]^{-1}\\left( \\begin{array}{c} \\beta _{0} \\\\ \\mu _{x _{1}} \\\\ \\mu _{x _{2}} \\\\ 0 \\end{array} \\right)\\\\ &amp;=\\left( \\begin{array}{c} \\beta _{0} + \\beta _{1} \\mu _{x _{1}} + \\beta _{2} \\mu _{x _{2}} \\\\ \\mu _{x _{1}} \\\\ \\mu _{x _{2}} \\end{array} \\right)\\end{align*}\\] 5.1.1 Using the ramR Package A ## y x1 x2 e ## y &quot;0&quot; &quot;beta[1]&quot; &quot;beta[2]&quot; &quot;1&quot; ## x1 &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## x2 &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## e &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; S ## y x1 x2 e ## y &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## x1 &quot;0&quot; &quot;sigma[x[1]]^2&quot; &quot;0&quot; &quot;0&quot; ## x2 &quot;0&quot; &quot;0&quot; &quot;sigma[x[2]]^2&quot; &quot;0&quot; ## e &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;sigma[varepsilon]^2&quot; u ## u ## y &quot;beta[0]&quot; ## x1 &quot;mu[x[1]]&quot; ## x2 &quot;mu[x[2]]&quot; ## e &quot;0&quot; Filter ## y x1 x2 e ## y 1 0 0 0 ## x1 0 1 0 0 ## x2 0 0 1 0 The covariance expectations can be symbolically derived using the ramR::C() function with A of class yac.symbol. ramR::C(Ryacas::ysym(A), S) ## {{sigma[x[1]]^2*beta[1]^2+sigma[x[2]]^2*beta[2]^2+sigma[varepsilon]^2, beta[1]*sigma[x[1]]^2, beta[2]*sigma[x[2]]^2, sigma[varepsilon]^2}, ## { sigma[x[1]]^2*beta[1], sigma[x[1]]^2, 0, 0}, ## { sigma[x[2]]^2*beta[2], 0, sigma[x[2]]^2, 0}, ## { sigma[varepsilon]^2, 0, 0, sigma[varepsilon]^2}} \\[\\begin{equation*}\\mathbf{C} =\\left( \\begin{array}{cccc} \\sigma _{x _{1}} ^{2} \\beta _{1} ^{2} + \\sigma _{x _{2}} ^{2} \\beta _{2} ^{2} + \\sigma _{\\varepsilon } ^{2} &amp; \\beta _{1} \\sigma _{x _{1}} ^{2} &amp; \\beta _{2} \\sigma _{x _{2}} ^{2} &amp; \\sigma _{\\varepsilon } ^{2} \\\\ \\sigma _{x _{1}} ^{2} \\beta _{1} &amp; \\sigma _{x _{1}} ^{2} &amp; 0 &amp; 0 \\\\ \\sigma _{x _{2}} ^{2} \\beta _{2} &amp; 0 &amp; \\sigma _{x _{2}} ^{2} &amp; 0 \\\\ \\sigma _{\\varepsilon } ^{2} &amp; 0 &amp; 0 &amp; \\sigma _{\\varepsilon } ^{2} \\end{array} \\right)\\end{equation*}\\] The covariance expectations for the observed variables can be symbolically derived using the ramR::M() function with A of class yac.symbol. ramR::M(Ryacas::ysym(A), S, Filter) ## {{sigma[x[1]]^2*beta[1]^2+sigma[x[2]]^2*beta[2]^2+sigma[varepsilon]^2, beta[1]*sigma[x[1]]^2, beta[2]*sigma[x[2]]^2}, ## { sigma[x[1]]^2*beta[1], sigma[x[1]]^2, 0}, ## { sigma[x[2]]^2*beta[2], 0, sigma[x[2]]^2}} \\[\\begin{equation*}\\mathbf{M} =\\left( \\begin{array}{ccc} \\sigma _{x _{1}} ^{2} \\beta _{1} ^{2} + \\sigma _{x _{2}} ^{2} \\beta _{2} ^{2} + \\sigma _{\\varepsilon } ^{2} &amp; \\beta _{1} \\sigma _{x _{1}} ^{2} &amp; \\beta _{2} \\sigma _{x _{2}} ^{2} \\\\ \\sigma _{x _{1}} ^{2} \\beta _{1} &amp; \\sigma _{x _{1}} ^{2} &amp; 0 \\\\ \\sigma _{x _{2}} ^{2} \\beta _{2} &amp; 0 &amp; \\sigma _{x _{2}} ^{2} \\end{array} \\right)\\end{equation*}\\] The mean expectations can be symbolically derived using the ramR::v() function with A of class yac.symbol. ramR::v(Ryacas::ysym(A), u) ## {{beta[0]+beta[1]*mu[x[1]]+beta[2]*mu[x[2]]}, ## { mu[x[1]]}, ## { mu[x[2]]}, ## { 0}} \\[\\begin{equation*}\\mathbf{v} =\\left( \\begin{array}{c} \\beta _{0} + \\beta _{1} \\mu _{x _{1}} + \\beta _{2} \\mu _{x _{2}} \\\\ \\mu _{x _{1}} \\\\ \\mu _{x _{2}} \\\\ 0 \\end{array} \\right)\\end{equation*}\\] The mean expectations for the observed variables can be symbolically derived using the ramR::g() function with A of class yac.symbol. ramR::g(Ryacas::ysym(A), u, Filter) ## {{beta[0]+beta[1]*mu[x[1]]+beta[2]*mu[x[2]]}, ## { mu[x[1]]}, ## { mu[x[2]]}} \\[\\begin{equation*}\\mathbf{g} =\\left( \\begin{array}{c} \\beta _{0} + \\beta _{1} \\mu _{x _{1}} + \\beta _{2} \\mu _{x _{2}} \\\\ \\mu _{x _{1}} \\\\ \\mu _{x _{2}} \\end{array} \\right)\\end{equation*}\\] 5.2 Numerical Example Let df be a random sample from a population with the following parameters Parameter \\(x = 0\\) \\(x = 1\\) \\(x = 2\\) Sample Size 500 500 500 \\(\\mathbb{E} \\left( y \\mid x \\right)\\) 0 2 1 \\(\\mathrm{Var} \\left( y \\mid x \\right)\\) 1 1 1 Parameter Description Value \\(\\beta_0\\) \\(\\mathbb{E} \\left( y \\mid x = 0 \\right)\\) 0 \\(\\beta_1\\) \\(\\mathbb{E} \\left( y \\mid x = 1 \\right) - \\mathbb{E} \\left( y \\mid x = 0 \\right)\\) 2 \\(\\beta_2\\) \\(\\mathbb{E} \\left( y \\mid x = 2 \\right) - \\mathbb{E} \\left( y \\mid x = 0 \\right)\\) 1 head(df) ## y x ## 1 -0.6013830 0 ## 2 -0.1358161 0 ## 3 -0.9872728 0 ## 4 0.8319250 0 ## 5 -0.7950595 0 ## 6 0.3404646 0 summary(df) ## y x ## Min. :-2.61364 0:500 ## 1st Qu.: 0.08094 1:500 ## Median : 1.02617 2:500 ## Mean : 1.00814 ## 3rd Qu.: 1.90112 ## Max. : 5.47091 5.2.1 One-Way Analysis of Variance Make sure that \\(x\\) is of class factor for lm and aov to treat it as a categorical variable. str(df) ## &#39;data.frame&#39;: 1500 obs. of 2 variables: ## $ y: num -0.601 -0.136 -0.987 0.832 -0.795 ... ## $ x: Factor w/ 3 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;: 1 1 1 1 1 1 1 1 1 1 ... summary(aov(y ~ x, data = df)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x 2 983.8 491.9 471.4 &lt;2e-16 *** ## Residuals 1497 1562.2 1.0 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 5.2.2 Linear Regression summary(lm(y ~ x, data = df)) ## ## Call: ## lm(formula = y ~ x, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.1792 -0.6469 0.0021 0.6751 3.5538 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.03083 0.04569 0.675 0.5 ## x1 1.98309 0.06461 30.694 &lt;2e-16 *** ## x2 0.94884 0.06461 14.686 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.022 on 1497 degrees of freedom ## Multiple R-squared: 0.3864, Adjusted R-squared: 0.3856 ## F-statistic: 471.4 on 2 and 1497 DF, p-value: &lt; 2.2e-16 5.2.3 Structural Equation Modeling We have to dummy code the data set first before fitting the model. The model.matrix function which is used to create a design matrix can be used to dummy code x. Make sure that x is a factor. The first column of the design matrix is a matrix of ones. Since we do not need this column, we can replace this column with the values of y. Make sure to name rename the first column as lavaan relies on the column names. df_dummy &lt;- model.matrix(y ~ x, data = df) df_dummy[, 1] &lt;- df$y colnames(df_dummy)[1] &lt;- &quot;y&quot; head(df_dummy) ## y x1 x2 ## 1 -0.6013830 0 0 ## 2 -0.1358161 0 0 ## 3 -0.9872728 0 0 ## 4 0.8319250 0 0 ## 5 -0.7950595 0 0 ## 6 0.3404646 0 0 5.2.3.1 lavaan (Rosseel 2012) model &lt;- &quot; y ~ x1 + x2 &quot; fit &lt;- lavaan::sem( model, data = df_dummy, meanstructure = TRUE, fixed.x = FALSE ) lavaan::summary(fit) ## lavaan 0.6-7 ended normally after 22 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 9 ## ## Number of observations 1500 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## y ~ ## x1 1.983 0.065 30.725 0.000 ## x2 0.949 0.065 14.701 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## x1 ~~ ## x2 -0.111 0.006 -17.321 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .y 0.031 0.046 0.676 0.499 ## x1 0.333 0.012 27.386 0.000 ## x2 0.333 0.012 27.386 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .y 1.041 0.038 27.386 0.000 ## x1 0.222 0.008 27.386 0.000 ## x2 0.222 0.008 27.386 0.000 5.2.3.2 OpenMx (Boker et al. 2020) RAM matrices can be used to specify models in OpenMx. Note, however, that the u vector in the RAM notation is M in the OpenMx notation. mxData &lt;- OpenMx::mxData( observed = df_dummy, type = &quot;raw&quot; ) mxA &lt;- OpenMx::mxMatrix( type = &quot;Full&quot;, nrow = 4, ncol = 4, free = c( F, T, T, F, F, F, F, F, F, F, F, F, F, F, F, F ), values = c( 0, 0.20, 0.20, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ), labels = c( NA, &quot;beta1&quot;, &quot;beta2&quot;, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA ), byrow = TRUE, name = &quot;mxA&quot; ) mxS &lt;- OpenMx::mxMatrix( type = &quot;Symm&quot;, nrow = 4, ncol = 4, free = c( F, F, F, F, F, T, F, F, F, F, T, F, F, F, F, T ), values = c( 0, 0, 0, 0, 0, 0.20, 0, 0, 0, 0, 0.20, 0, 0, 0, 0, 0.20 ), labels = c( NA, NA, NA, NA, NA, &quot;sigma2x1&quot;, NA, NA, NA, NA, &quot;sigma2x2&quot;, NA, NA, NA, NA, &quot;sigma2e&quot; ), byrow = TRUE, name = &quot;mxS&quot; ) mxM &lt;- OpenMx::mxMatrix( type = &quot;Full&quot;, nrow = 1, ncol = 4, free = c( T, T, T, F ), values = c( 0.20, 0.20, 0.20, 0 ), labels = c( &quot;beta0&quot;, &quot;mux1&quot;, &quot;mux2&quot;, NA ), byrow = TRUE, name = &quot;mxM&quot; ) mxF &lt;- OpenMx::mxMatrix( type = &quot;Full&quot;, nrow = 3, ncol = 4, free = FALSE, values = c( 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0 ), byrow = TRUE, name = &quot;mxF&quot; ) expRAM &lt;- OpenMx::mxExpectationRAM( A = &quot;mxA&quot;, S = &quot;mxS&quot;, F = &quot;mxF&quot;, M = &quot;mxM&quot;, dimnames = c( &quot;y&quot;, &quot;x1&quot;, &quot;x2&quot;, &quot;e&quot; ) ) objML &lt;- OpenMx::mxFitFunctionML() mxMod &lt;- OpenMx::mxModel( name = &quot;One Way Analysis of Variance&quot;, data = mxData, matrices = list( mxA, mxS, mxF, mxM ), expectation = expRAM, fitfunction = objML ) fit &lt;- OpenMx::mxRun(mxMod) ## Running One Way Analysis of Variance with 8 parameters summary(fit) ## Summary of One Way Analysis of Variance ## ## free parameters: ## name matrix row col Estimate Std.Error A ## 1 beta1 mxA 1 2 1.98308662 0.064543779 ## 2 beta2 mxA 1 3 0.94883814 0.064543143 ## 3 sigma2x1 mxS 2 2 0.22222230 0.008114416 ## 4 sigma2x2 mxS 3 3 0.22222238 0.008114420 ## 5 sigma2e mxS 4 4 1.04147460 0.038029458 ## 6 beta0 mxM 1 y 0.03083127 0.045639092 ## 7 mux1 mxM 1 x1 0.33333343 0.012171613 ## 8 mux2 mxM 1 x2 0.33333344 0.012171612 ## ## Model Statistics: ## | Parameters | Degrees of Freedom | Fit (-2lnL units) ## Model: 8 4492 8319.17 ## Saturated: 9 4491 NA ## Independence: 6 4494 NA ## Number of observations/statistics: 1500/4500 ## ## Information Criteria: ## | df Penalty | Parameters Penalty | Sample-Size Adjusted ## AIC: -664.8302 8335.170 8335.266 ## BIC: -24531.8162 8377.676 8352.262 ## To get additional fit indices, see help(mxRefModels) ## timestamp: 2021-02-14 03:27:15 ## Wall clock time: 0.03418255 secs ## optimizer: SLSQP ## OpenMx version number: 2.18.1 ## Need help? See help(mxSummary) 5.2.4 Using the ramR Package A ## y x1 x2 e ## y 0 2.008444 0.9885797 1 ## x1 0 0.000000 0.0000000 0 ## x2 0 0.000000 0.0000000 0 ## e 0 0.000000 0.0000000 0 S ## y x1 x2 e ## y 0 0.0000000 0.0000000 0.0000000 ## x1 0 0.2223705 0.0000000 0.0000000 ## x2 0 0.0000000 0.2223705 0.0000000 ## e 0 0.0000000 0.0000000 0.9823083 u ## u ## y 0.3333333 ## x1 0.3333333 ## x2 0.3333333 ## e 0.0000000 Filter ## y x1 x2 e ## y 1 0 0 0 ## x1 0 1 0 0 ## x2 0 0 1 0 The covariance expectations can be numerically derived using the ramR::C() function. ramR::C(A, S) ## y x1 x2 e ## y 2.0966368 0.4466185 0.2198309 0.9823083 ## x1 0.4466185 0.2223705 0.0000000 0.0000000 ## x2 0.2198309 0.0000000 0.2223705 0.0000000 ## e 0.9823083 0.0000000 0.0000000 0.9823083 The covariance expectations for the observed variables can be numerically derived using the ramR::M() function. ramR::M(A, S, Filter) ## y x1 x2 ## y 2.0966368 0.4466185 0.2198309 ## x1 0.4466185 0.2223705 0.0000000 ## x2 0.2198309 0.0000000 0.2223705 The mean expectations can be numerically derived using the ramR::v() function. ramR::v(A, u) ## v ## y 1.3323411 ## x1 0.3333333 ## x2 0.3333333 ## e 0.0000000 The mean expectations for the observed variables can be numerically derived using the ramR::v() function. ramR::g(A, u, Filter) ## g ## y 1.3323411 ## x1 0.3333333 ## x2 0.3333333 5.3 Equations to RAM The ramR package has a utility function to convert structural equations to RAM notation. One-way analysis of variance with three levels can be expressed in the following equations eq &lt;- &quot; # VARIABLE1 OPERATION VARIABLE2 LABEL e by y 1 y on x1 beta[1] y on x2 beta[2] e with e sigma[varepsilon]^2 x1 with x1 sigma[x[1]]^2 x2 with x2 sigma[x[2]]^2 y on 1 beta[0] x1 on 1 mu[x[1]] x2 on 1 mu[x[2]] &quot; Figure 5.2: One-Way Analysis of Variance’s Structural Equations The error term is treated as a latent variable and defined with the operation by. Its value is constrained to \\(1\\). The regression of \\(y\\) on \\(x_1\\) and \\(x_2\\) is defined by operation on. The coefficients are labeled as beta[1] and beta[2] respectively. The variance of \\(x_1\\), \\(x_2\\) and the error variance are defined using the operation with. These are labeled sigma[x[1]]^2, sigma[x[2]]^2, and sigma[varepsilon]^2 respectively. The intercept and the mean of \\(x_1\\) and \\(x_2\\) are defined using the operation on 1. These are labeled beta[0], mu[x[1]], and mu[x[2]] respectively. The ramR::Eq2RAM converts the equations to RAM notation. ramR::Eq2RAM(eq) ## $par.table ## lhs op rhs par.label par.index ## 1 e by y 1 1 ## 2 y on x1 beta[1] p1 ## 3 y on x2 beta[2] p2 ## 4 e with e sigma[varepsilon]^2 p3 ## 5 x1 with x1 sigma[x[1]]^2 p4 ## 6 x2 with x2 sigma[x[2]]^2 p5 ## 7 y on 1 beta[0] p6 ## 8 x1 on 1 mu[x[1]] p7 ## 9 x2 on 1 mu[x[2]] p8 ## ## $variables ## [1] &quot;y&quot; &quot;x1&quot; &quot;x2&quot; &quot;e&quot; ## ## $g.variables ## [1] &quot;y&quot; &quot;x1&quot; &quot;x2&quot; ## ## $h.variables ## [1] &quot;e&quot; ## ## $A ## y x1 x2 e ## y &quot;0&quot; &quot;beta[1]&quot; &quot;beta[2]&quot; &quot;1&quot; ## x1 &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## x2 &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## e &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## ## $S ## y x1 x2 e ## y &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## x1 &quot;0&quot; &quot;sigma[x[1]]^2&quot; &quot;0&quot; &quot;0&quot; ## x2 &quot;0&quot; &quot;0&quot; &quot;sigma[x[2]]^2&quot; &quot;0&quot; ## e &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;sigma[varepsilon]^2&quot; ## ## $u ## u ## y &quot;beta[0]&quot; ## x1 &quot;mu[x[1]]&quot; ## x2 &quot;mu[x[2]]&quot; ## e &quot;0&quot; ## ## $Filter ## y x1 x2 e ## y 1 0 0 0 ## x1 0 1 0 0 ## x2 0 0 1 0 5.4 Equations to Expectations The ramR package has a utility function to convert structural equations to expectations both symbolically and numerically. eq &lt;- &quot; # VARIABLE1 OPERATION VARIABLE2 LABEL e by y 1 y on x1 beta[1] y on x2 beta[2] e with e sigma[varepsilon]^2 x1 with x1 sigma[x[1]]^2 x2 with x2 sigma[x[2]]^2 y on 1 beta[0] x1 on 1 mu[x[1]] x2 on 1 mu[x[2]] &quot; ramR::Eq2Expectations(eq) ## $par.table ## lhs op rhs par.label par.index ## 1 e by y 1 1 ## 2 y on x1 beta[1] p1 ## 3 y on x2 beta[2] p2 ## 4 e with e sigma[varepsilon]^2 p3 ## 5 x1 with x1 sigma[x[1]]^2 p4 ## 6 x2 with x2 sigma[x[2]]^2 p5 ## 7 y on 1 beta[0] p6 ## 8 x1 on 1 mu[x[1]] p7 ## 9 x2 on 1 mu[x[2]] p8 ## ## $variables ## [1] &quot;y&quot; &quot;x1&quot; &quot;x2&quot; &quot;e&quot; ## ## $g.variables ## [1] &quot;y&quot; &quot;x1&quot; &quot;x2&quot; ## ## $h.variables ## [1] &quot;e&quot; ## ## $A ## {{ 0, beta[1], beta[2], 1}, ## { 0, 0, 0, 0}, ## { 0, 0, 0, 0}, ## { 0, 0, 0, 0}} ## ## $S ## {{ 0, 0, 0, 0}, ## { 0, sigma[x[1]]^2, 0, 0}, ## { 0, 0, sigma[x[2]]^2, 0}, ## { 0, 0, 0, sigma[varepsilon]^2}} ## ## $u ## {{ beta[0]}, ## {mu[x[1]]}, ## {mu[x[2]]}, ## { 0}} ## ## $Filter ## {{1, 0, 0, 0}, ## {0, 1, 0, 0}, ## {0, 0, 1, 0}} ## ## $v ## {{beta[0]+beta[1]*mu[x[1]]+beta[2]*mu[x[2]]}, ## { mu[x[1]]}, ## { mu[x[2]]}, ## { 0}} ## ## $g ## {{beta[0]+beta[1]*mu[x[1]]+beta[2]*mu[x[2]]}, ## { mu[x[1]]}, ## { mu[x[2]]}} ## ## $C ## {{sigma[x[1]]^2*beta[1]^2+sigma[x[2]]^2*beta[2]^2+sigma[varepsilon]^2, beta[1]*sigma[x[1]]^2, beta[2]*sigma[x[2]]^2, sigma[varepsilon]^2}, ## { sigma[x[1]]^2*beta[1], sigma[x[1]]^2, 0, 0}, ## { sigma[x[2]]^2*beta[2], 0, sigma[x[2]]^2, 0}, ## { sigma[varepsilon]^2, 0, 0, sigma[varepsilon]^2}} ## ## $M ## {{sigma[x[1]]^2*beta[1]^2+sigma[x[2]]^2*beta[2]^2+sigma[varepsilon]^2, beta[1]*sigma[x[1]]^2, beta[2]*sigma[x[2]]^2}, ## { sigma[x[1]]^2*beta[1], sigma[x[1]]^2, 0}, ## { sigma[x[2]]^2*beta[2], 0, sigma[x[2]]^2}} eq &lt;- &quot; # VARIABLE1 OPERATION VARIABLE2 LABEL e by y 1 y on x1 2 y on x2 1 e with e 1 x1 with x1 0.22222222222 x2 with x2 0.22222222222 y on 1 0 x1 on 1 0.33333333333 x2 on 1 0.33333333333 &quot; ramR::Eq2Expectations(eq) ## $par.table ## lhs op rhs par.label par.index ## 1 e by y 1.0000000 1.0000000 ## 2 y on x1 2.0000000 2.0000000 ## 3 y on x2 1.0000000 1.0000000 ## 4 e with e 1.0000000 1.0000000 ## 5 x1 with x1 0.2222222 0.2222222 ## 6 x2 with x2 0.2222222 0.2222222 ## 7 y on 1 0.0000000 0.0000000 ## 8 x1 on 1 0.3333333 0.3333333 ## 9 x2 on 1 0.3333333 0.3333333 ## ## $variables ## [1] &quot;y&quot; &quot;x1&quot; &quot;x2&quot; &quot;e&quot; ## ## $g.variables ## [1] &quot;y&quot; &quot;x1&quot; &quot;x2&quot; ## ## $h.variables ## [1] &quot;e&quot; ## ## $A ## y x1 x2 e ## y 0 2 1 1 ## x1 0 0 0 0 ## x2 0 0 0 0 ## e 0 0 0 0 ## ## $S ## y x1 x2 e ## y 0 0.0000000 0.0000000 0 ## x1 0 0.2222222 0.0000000 0 ## x2 0 0.0000000 0.2222222 0 ## e 0 0.0000000 0.0000000 1 ## ## $u ## u ## y 0.0000000 ## x1 0.3333333 ## x2 0.3333333 ## e 0.0000000 ## ## $Filter ## y x1 x2 e ## y 1 0 0 0 ## x1 0 1 0 0 ## x2 0 0 1 0 ## ## $v ## v ## y 1.0000000 ## x1 0.3333333 ## x2 0.3333333 ## e 0.0000000 ## ## $g ## g ## y 1.0000000 ## x1 0.3333333 ## x2 0.3333333 ## ## $C ## y x1 x2 e ## y 2.1111111 0.4444444 0.2222222 1 ## x1 0.4444444 0.2222222 0.0000000 0 ## x2 0.2222222 0.0000000 0.2222222 0 ## e 1.0000000 0.0000000 0.0000000 1 ## ## $M ## y x1 x2 ## y 2.1111111 0.4444444 0.2222222 ## x1 0.4444444 0.2222222 0.0000000 ## x2 0.2222222 0.0000000 0.2222222 "],["ram-two-reg.html", "Chapter 6 Two-Variable Regression Model", " Chapter 6 Two-Variable Regression Model Figure 6.1: Two-Variable Regression Model "],["ram-k-reg.html", "Chapter 7 \\(k\\)-Variable Regression Model", " Chapter 7 \\(k\\)-Variable Regression Model Figure 7.1: \\(k\\)-Variable Regression Model "],["ram-med-simple.html", "Chapter 8 The Simple Mediation Model 8.1 Symbolic 8.2 Numerical Example 8.3 Equations to RAM 8.4 Equations to Expectations", " Chapter 8 The Simple Mediation Model Let \\(y\\), \\(m\\), \\(x\\), \\(\\varepsilon_y\\), and \\(\\varepsilon_m\\) be random variables whose associations are given by \\[\\begin{equation} y = \\beta_0 + \\beta_1 x + \\beta_2 m + \\varepsilon_y \\end{equation}\\] \\[\\begin{equation} m = \\alpha_0 + \\alpha_1 x + \\varepsilon_m \\end{equation}\\] or combined \\[\\begin{equation} \\begin{split} y &amp;= \\beta_0 + \\beta_1 x + \\beta_2 (\\alpha_0 + \\alpha_1 x + \\varepsilon_m) + \\varepsilon_y \\\\ &amp;= \\beta_0 + \\beta_1 x + \\beta_2 \\alpha_0 + \\alpha_1 \\beta_2 x + \\beta_2 \\varepsilon_m + \\varepsilon_y \\end{split} \\end{equation}\\] where \\(\\beta_1\\) is the path from \\(x\\) on \\(y\\) \\(\\beta_2\\) is the path from \\(m\\) to \\(y\\) \\(\\alpha_1\\) is the path from \\(x\\) to \\(m\\) \\(\\varepsilon_y\\) and \\(\\varepsilon_m\\) are uncorrelated error terms with means of zero and constant variances of \\(\\sigma_{\\varepsilon_y}^{2}\\) and \\(\\sigma_{\\varepsilon_m}^{2}\\) respectively \\(\\beta_0\\) and \\(\\alpha_0\\) are intercepts Figure 8.1: The Simple Mediation Model Figure 8.2: The Simple Mediation Model with Mean Structure 8.1 Symbolic Let \\(\\left\\{ y, m, x, \\varepsilon_y, \\varepsilon_m \\right\\}\\) be the variables of interest. \\[\\begin{align*}\\mathbf{A} &amp;=\\left( \\begin{array}{ccccc} 0 &amp; \\beta _{2} &amp; \\beta _{1} &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; \\alpha _{1} &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{S} &amp;=\\left( \\begin{array}{ccccc} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma _{x} ^{2} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\sigma _{\\varepsilon _{y}} ^{2} &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\sigma _{\\varepsilon _{m}} ^{2} \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{C} &amp;=\\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\mathbf{S} \\left[ \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\right]^{\\mathsf{T}} \\\\\\\\ &amp;=\\mathbf{E} \\mathbf{S} \\mathbf{E}^{\\mathsf{T}} \\\\\\\\ &amp;=\\left( \\begin{array}{ccccc} 1 &amp; \\beta _{2} &amp; \\beta _{1} + \\beta _{2} \\alpha _{1} &amp; 1 &amp; \\beta _{2} \\\\ 0 &amp; 1 &amp; \\alpha _{1} &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\end{array} \\right)\\left( \\begin{array}{ccccc} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma _{x} ^{2} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\sigma _{\\varepsilon _{y}} ^{2} &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\sigma _{\\varepsilon _{m}} ^{2} \\end{array} \\right)\\left( \\begin{array}{ccccc} 1 &amp; \\beta _{2} &amp; \\beta _{1} + \\beta _{2} \\alpha _{1} &amp; 1 &amp; \\beta _{2} \\\\ 0 &amp; 1 &amp; \\alpha _{1} &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\end{array} \\right)^{\\mathsf{T}}\\\\ &amp;=\\left( \\begin{array}{ccccc} \\sigma _{x} ^{2} \\beta _{1} ^{2} + 2 \\sigma _{x} ^{2} \\beta _{1} \\beta _{2} \\alpha _{1} + \\sigma _{x} ^{2} \\beta _{2} ^{2} \\alpha _{1} ^{2} + \\beta _{2} ^{2} \\sigma _{\\varepsilon _{m}} ^{2} + \\sigma _{\\varepsilon _{y}} ^{2} &amp; \\beta _{1} \\alpha _{1} \\sigma _{x} ^{2} + \\beta _{2} \\alpha _{1} ^{2} \\sigma _{x} ^{2} + \\beta _{2} \\sigma _{\\varepsilon _{m}} ^{2} &amp; \\beta _{1} \\sigma _{x} ^{2} + \\beta _{2} \\alpha _{1} \\sigma _{x} ^{2} &amp; \\sigma _{\\varepsilon _{y}} ^{2} &amp; \\beta _{2} \\sigma _{\\varepsilon _{m}} ^{2} \\\\ \\alpha _{1} ^{2} \\sigma _{x} ^{2} \\beta _{2} + \\alpha _{1} \\sigma _{x} ^{2} \\beta _{1} + \\beta _{2} \\sigma _{\\varepsilon _{m}} ^{2} &amp; \\sigma _{x} ^{2} \\alpha _{1} ^{2} + \\sigma _{\\varepsilon _{m}} ^{2} &amp; \\alpha _{1} \\sigma _{x} ^{2} &amp; 0 &amp; \\sigma _{\\varepsilon _{m}} ^{2} \\\\ \\sigma _{x} ^{2} \\beta _{1} + \\sigma _{x} ^{2} \\beta _{2} \\alpha _{1} &amp; \\sigma _{x} ^{2} \\alpha _{1} &amp; \\sigma _{x} ^{2} &amp; 0 &amp; 0 \\\\ \\sigma _{\\varepsilon _{y}} ^{2} &amp; 0 &amp; 0 &amp; \\sigma _{\\varepsilon _{y}} ^{2} &amp; 0 \\\\ \\sigma _{\\varepsilon _{m}} ^{2} \\beta _{2} &amp; \\sigma _{\\varepsilon _{m}} ^{2} &amp; 0 &amp; 0 &amp; \\sigma _{\\varepsilon _{m}} ^{2} \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{F} &amp;=\\left( \\begin{array}{ccccc} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{M} &amp;=\\mathbf{F} \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\mathbf{S} \\left[ \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\right]^{\\mathsf{T}} \\mathbf{F}^{\\mathsf{T}} \\\\&amp;=\\mathbf{F} \\mathbf{E} \\mathbf{S} \\mathbf{E}^{\\mathsf{T}} \\mathbf{F}^{\\mathsf{T}} \\\\&amp;=\\mathbf{F} \\mathbf{C} \\mathbf{F}^{\\mathsf{T}} \\\\&amp;=\\left( \\begin{array}{ccc} \\sigma _{x} ^{2} \\beta _{1} ^{2} + 2 \\sigma _{x} ^{2} \\beta _{1} \\beta _{2} \\alpha _{1} + \\sigma _{x} ^{2} \\beta _{2} ^{2} \\alpha _{1} ^{2} + \\beta _{2} ^{2} \\sigma _{\\varepsilon _{m}} ^{2} + \\sigma _{\\varepsilon _{y}} ^{2} &amp; \\beta _{1} \\alpha _{1} \\sigma _{x} ^{2} + \\beta _{2} \\alpha _{1} ^{2} \\sigma _{x} ^{2} + \\beta _{2} \\sigma _{\\varepsilon _{m}} ^{2} &amp; \\beta _{1} \\sigma _{x} ^{2} + \\beta _{2} \\alpha _{1} \\sigma _{x} ^{2} \\\\ \\alpha _{1} ^{2} \\sigma _{x} ^{2} \\beta _{2} + \\alpha _{1} \\sigma _{x} ^{2} \\beta _{1} + \\beta _{2} \\sigma _{\\varepsilon _{m}} ^{2} &amp; \\sigma _{x} ^{2} \\alpha _{1} ^{2} + \\sigma _{\\varepsilon _{m}} ^{2} &amp; \\alpha _{1} \\sigma _{x} ^{2} \\\\ \\sigma _{x} ^{2} \\beta _{1} + \\sigma _{x} ^{2} \\beta _{2} \\alpha _{1} &amp; \\sigma _{x} ^{2} \\alpha _{1} &amp; \\sigma _{x} ^{2} \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{v} &amp;=\\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\mathbf{u}\\\\ &amp;=\\left[\\left( \\begin{array}{ccccc} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\end{array} \\right)-\\left( \\begin{array}{ccccc} 0 &amp; \\beta _{2} &amp; \\beta _{1} &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; \\alpha _{1} &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{array} \\right)\\right]^{\\mathsf{-1}}\\left( \\begin{array}{c} \\beta _{0} \\\\ \\alpha _{0} \\\\ \\mu _{x} \\\\ 0 \\\\ 0 \\end{array} \\right)\\\\ &amp;=\\left( \\begin{array}{c} \\beta _{0} + \\beta _{2} \\alpha _{0} + \\beta _{2} \\alpha _{1} \\mu _{x} + \\beta _{1} \\mu _{x} \\\\ \\alpha _{0} + \\alpha _{1} \\mu _{x} \\\\ \\mu _{x} \\\\ 0 \\\\ 0 \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{u} &amp;=\\left( \\mathbf{I} - \\mathbf{A} \\right) \\mathbf{v}\\\\ &amp;=\\left[\\left( \\begin{array}{ccccc} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\end{array} \\right)-\\left( \\begin{array}{ccccc} 0 &amp; \\beta _{2} &amp; \\beta _{1} &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; \\alpha _{1} &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{array} \\right)\\right]\\left( \\begin{array}{c} \\beta _{0} + \\beta _{1} \\alpha _{0} + \\beta _{1} \\alpha _{1} \\mu _{x} + \\beta _{2} \\mu _{x} \\\\ \\alpha _{0} + \\alpha _{1} \\mu _{x} \\\\ \\mu _{x} \\\\ 0 \\\\ 0 \\end{array} \\right)\\\\ &amp;=\\left( \\begin{array}{c} \\beta _{0} + \\beta _{1} \\alpha _{0} + \\beta _{1} \\alpha _{1} \\mu _{x} + \\mu _{x} \\beta _{2} \\\\ \\alpha _{0} + \\alpha _{1} \\mu _{x} \\\\ \\mu _{x} \\\\ 0 \\\\ 0 \\end{array} \\right)\\end{align*}\\] \\[\\begin{align*}\\mathbf{g} &amp;=\\mathbf{F} \\left( \\mathbf{I} - \\mathbf{A} \\right)^{-1} \\mathbf{u}\\\\ &amp;=\\left[\\left( \\begin{array}{ccccc} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\end{array} \\right)-\\left( \\begin{array}{ccccc} 0 &amp; \\beta _{2} &amp; \\beta _{1} &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; \\alpha _{1} &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{array} \\right)\\right]^{-1}\\left( \\begin{array}{c} \\beta _{0} \\\\ \\alpha _{0} \\\\ \\mu _{x} \\\\ 0 \\\\ 0 \\end{array} \\right)\\\\ &amp;=\\left( \\begin{array}{c} \\beta _{0} + \\beta _{2} \\alpha _{0} + \\beta _{2} \\alpha _{1} \\mu _{x} + \\beta _{1} \\mu _{x} \\\\ \\alpha _{0} + \\alpha _{1} \\mu _{x} \\\\ \\mu _{x} \\end{array} \\right)\\end{align*}\\] 8.1.1 Using the ramR Package A ## y m x ey em ## y &quot;0&quot; &quot;beta[2]&quot; &quot;beta[1]&quot; &quot;1&quot; &quot;0&quot; ## m &quot;0&quot; &quot;0&quot; &quot;alpha[1]&quot; &quot;0&quot; &quot;1&quot; ## x &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## ey &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## em &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; S ## y m x ey em ## y &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## m &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## x &quot;0&quot; &quot;0&quot; &quot;sigma[x]^2&quot; &quot;0&quot; &quot;0&quot; ## ey &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;sigma[varepsilon[y]]^2&quot; &quot;0&quot; ## em &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;sigma[varepsilon[m]]^2&quot; u ## u ## y &quot;beta[0]&quot; ## m &quot;alpha[0]&quot; ## x &quot;mu[x]&quot; ## ey &quot;0&quot; ## em &quot;0&quot; filter ## y m x ey em ## y 1 0 0 0 0 ## m 0 1 0 0 0 ## x 0 0 1 0 0 The covariance expectations can be symbolically derived using the ramR::C() function with A of class yac.symbol. ramR::C(Ryacas::ysym(A), S, simplify = TRUE) ## {{sigma[x]^2*beta[1]^2+2*sigma[x]^2*beta[1]*beta[2]*alpha[1]+sigma[x]^2*beta[2]^2*alpha[1]^2+beta[2]^2*sigma[varepsilon[m]]^2+sigma[varepsilon[y]]^2, beta[1]*alpha[1]*sigma[x]^2+beta[2]*alpha[1]^2*sigma[x]^2+beta[2]*sigma[varepsilon[m]]^2, beta[1]*sigma[x]^2+beta[2]*alpha[1]*sigma[x]^2, sigma[varepsilon[y]]^2, beta[2]*sigma[varepsilon[m]]^2}, ## { alpha[1]^2*sigma[x]^2*beta[2]+alpha[1]*sigma[x]^2*beta[1]+beta[2]*sigma[varepsilon[m]]^2, sigma[x]^2*alpha[1]^2+sigma[varepsilon[m]]^2, alpha[1]*sigma[x]^2, 0, sigma[varepsilon[m]]^2}, ## { sigma[x]^2*beta[1]+sigma[x]^2*beta[2]*alpha[1], sigma[x]^2*alpha[1], sigma[x]^2, 0, 0}, ## { sigma[varepsilon[y]]^2, 0, 0, sigma[varepsilon[y]]^2, 0}, ## { sigma[varepsilon[m]]^2*beta[2], sigma[varepsilon[m]]^2, 0, 0, sigma[varepsilon[m]]^2}} \\[\\begin{equation*}\\mathbf{C} =\\left( \\begin{array}{ccccc} \\sigma _{x} ^{2} \\beta _{1} ^{2} + 2 \\sigma _{x} ^{2} \\beta _{1} \\beta _{2} \\alpha _{1} + \\sigma _{x} ^{2} \\beta _{2} ^{2} \\alpha _{1} ^{2} + \\beta _{2} ^{2} \\sigma _{\\varepsilon _{m}} ^{2} + \\sigma _{\\varepsilon _{y}} ^{2} &amp; \\beta _{1} \\alpha _{1} \\sigma _{x} ^{2} + \\beta _{2} \\alpha _{1} ^{2} \\sigma _{x} ^{2} + \\beta _{2} \\sigma _{\\varepsilon _{m}} ^{2} &amp; \\beta _{1} \\sigma _{x} ^{2} + \\beta _{2} \\alpha _{1} \\sigma _{x} ^{2} &amp; \\sigma _{\\varepsilon _{y}} ^{2} &amp; \\beta _{2} \\sigma _{\\varepsilon _{m}} ^{2} \\\\ \\alpha _{1} ^{2} \\sigma _{x} ^{2} \\beta _{2} + \\alpha _{1} \\sigma _{x} ^{2} \\beta _{1} + \\beta _{2} \\sigma _{\\varepsilon _{m}} ^{2} &amp; \\sigma _{x} ^{2} \\alpha _{1} ^{2} + \\sigma _{\\varepsilon _{m}} ^{2} &amp; \\alpha _{1} \\sigma _{x} ^{2} &amp; 0 &amp; \\sigma _{\\varepsilon _{m}} ^{2} \\\\ \\sigma _{x} ^{2} \\beta _{1} + \\sigma _{x} ^{2} \\beta _{2} \\alpha _{1} &amp; \\sigma _{x} ^{2} \\alpha _{1} &amp; \\sigma _{x} ^{2} &amp; 0 &amp; 0 \\\\ \\sigma _{\\varepsilon _{y}} ^{2} &amp; 0 &amp; 0 &amp; \\sigma _{\\varepsilon _{y}} ^{2} &amp; 0 \\\\ \\sigma _{\\varepsilon _{m}} ^{2} \\beta _{2} &amp; \\sigma _{\\varepsilon _{m}} ^{2} &amp; 0 &amp; 0 &amp; \\sigma _{\\varepsilon _{m}} ^{2} \\end{array} \\right)\\end{equation*}\\] The covariance expectations for the observed variables can be symbolically derived using the ramR::M() function with A of class yac.symbol. ramR::M(Ryacas::ysym(A), S, filter, simplify = TRUE) ## {{sigma[x]^2*beta[1]^2+2*sigma[x]^2*beta[1]*beta[2]*alpha[1]+sigma[x]^2*beta[2]^2*alpha[1]^2+beta[2]^2*sigma[varepsilon[m]]^2+sigma[varepsilon[y]]^2, beta[1]*alpha[1]*sigma[x]^2+beta[2]*alpha[1]^2*sigma[x]^2+beta[2]*sigma[varepsilon[m]]^2, beta[1]*sigma[x]^2+beta[2]*alpha[1]*sigma[x]^2}, ## { alpha[1]^2*sigma[x]^2*beta[2]+alpha[1]*sigma[x]^2*beta[1]+beta[2]*sigma[varepsilon[m]]^2, sigma[x]^2*alpha[1]^2+sigma[varepsilon[m]]^2, alpha[1]*sigma[x]^2}, ## { sigma[x]^2*beta[1]+sigma[x]^2*beta[2]*alpha[1], sigma[x]^2*alpha[1], sigma[x]^2}} \\[\\begin{equation*}\\mathbf{M} =\\left( \\begin{array}{ccc} \\sigma _{x} ^{2} \\beta _{1} ^{2} + 2 \\sigma _{x} ^{2} \\beta _{1} \\beta _{2} \\alpha _{1} + \\sigma _{x} ^{2} \\beta _{2} ^{2} \\alpha _{1} ^{2} + \\beta _{2} ^{2} \\sigma _{\\varepsilon _{m}} ^{2} + \\sigma _{\\varepsilon _{y}} ^{2} &amp; \\beta _{1} \\alpha _{1} \\sigma _{x} ^{2} + \\beta _{2} \\alpha _{1} ^{2} \\sigma _{x} ^{2} + \\beta _{2} \\sigma _{\\varepsilon _{m}} ^{2} &amp; \\beta _{1} \\sigma _{x} ^{2} + \\beta _{2} \\alpha _{1} \\sigma _{x} ^{2} \\\\ \\alpha _{1} ^{2} \\sigma _{x} ^{2} \\beta _{2} + \\alpha _{1} \\sigma _{x} ^{2} \\beta _{1} + \\beta _{2} \\sigma _{\\varepsilon _{m}} ^{2} &amp; \\sigma _{x} ^{2} \\alpha _{1} ^{2} + \\sigma _{\\varepsilon _{m}} ^{2} &amp; \\alpha _{1} \\sigma _{x} ^{2} \\\\ \\sigma _{x} ^{2} \\beta _{1} + \\sigma _{x} ^{2} \\beta _{2} \\alpha _{1} &amp; \\sigma _{x} ^{2} \\alpha _{1} &amp; \\sigma _{x} ^{2} \\end{array} \\right)\\end{equation*}\\] The mean expectations can be symbolically derived using the ramR::v() function with A of class yac.symbol. ramR::v(Ryacas::ysym(A), u, simplify = TRUE) ## {{beta[0]+beta[2]*alpha[0]+beta[2]*alpha[1]*mu[x]+beta[1]*mu[x]}, ## { alpha[0]+alpha[1]*mu[x]}, ## { mu[x]}, ## { 0}, ## { 0}} \\[\\begin{equation*}\\mathbf{v} =\\left( \\begin{array}{c} \\beta _{0} + \\beta _{2} \\alpha _{0} + \\beta _{2} \\alpha _{1} \\mu _{x} + \\beta _{1} \\mu _{x} \\\\ \\alpha _{0} + \\alpha _{1} \\mu _{x} \\\\ \\mu _{x} \\\\ 0 \\\\ 0 \\end{array} \\right)\\end{equation*}\\] The mean expectations for the observed variables can be symbolically derived using the ramR::g() function with A of class yac.symbol. ramR::g(Ryacas::ysym(A), u, filter, simplify = TRUE) ## {{beta[0]+beta[2]*alpha[0]+beta[2]*alpha[1]*mu[x]+beta[1]*mu[x]}, ## { alpha[0]+alpha[1]*mu[x]}, ## { mu[x]}} \\[\\begin{equation*}\\mathbf{g} =\\left( \\begin{array}{c} \\beta _{0} + \\beta _{2} \\alpha _{0} + \\beta _{2} \\alpha _{1} \\mu _{x} + \\beta _{1} \\mu _{x} \\\\ \\alpha _{0} + \\alpha _{1} \\mu _{x} \\\\ \\mu _{x} \\end{array} \\right)\\end{equation*}\\] 8.2 Numerical Example Let df be a random sample from a population with the following parameters Parameter Value \\(\\beta_1\\) 0 \\(\\beta_2\\) 0.5 \\(\\alpha_1\\) 0.5 \\(\\sigma_{\\varepsilon_y}^{2}\\) 168.75 \\(\\sigma_{\\varepsilon_m}^{2}\\) 168.75 \\(\\sigma_{x}^{2}\\) 225 \\(\\beta_0\\) 50 \\(\\alpha_0\\) 50 \\(\\mu_x\\) 100 head(df) ## y m x ## 1 107.32121 80.22215 64.60716 ## 2 109.83608 109.42737 100.20734 ## 3 97.41131 107.89265 79.57780 ## 4 87.75538 106.43004 80.82994 ## 5 80.77481 104.18033 99.07128 ## 6 97.27327 98.70669 108.24974 summary(df) ## y m x ## Min. : 47.09 Min. : 55.30 Min. : 45.16 ## 1st Qu.: 90.02 1st Qu.: 90.39 1st Qu.: 90.09 ## Median :100.34 Median : 99.84 Median :100.14 ## Mean :100.22 Mean :100.36 Mean :100.32 ## 3rd Qu.:110.30 3rd Qu.:110.40 3rd Qu.:110.29 ## Max. :149.73 Max. :150.48 Max. :156.91 8.2.1 Linear Regression summary(lm(y ~ x + m, data = df)) ## ## Call: ## lm(formula = y ~ x + m, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -42.413 -8.837 -0.045 8.804 39.148 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 48.47917 3.19559 15.171 &lt;2e-16 *** ## x 0.04486 0.03101 1.447 0.148 ## m 0.47072 0.03136 15.012 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 12.98 on 997 degrees of freedom ## Multiple R-squared: 0.2464, Adjusted R-squared: 0.2449 ## F-statistic: 163 on 2 and 997 DF, p-value: &lt; 2.2e-16 summary(lm(m ~ x, data = df)) ## ## Call: ## lm(formula = m ~ x, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -43.182 -9.426 0.280 8.705 38.824 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 52.04456 2.77350 18.77 &lt;2e-16 *** ## x 0.48163 0.02734 17.62 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 13.1 on 998 degrees of freedom ## Multiple R-squared: 0.2372, Adjusted R-squared: 0.2365 ## F-statistic: 310.4 on 1 and 998 DF, p-value: &lt; 2.2e-16 8.2.2 Structural Equation Modeling 8.2.2.1 lavaan (Rosseel 2012) # Covariance Structure model &lt;- &quot; y ~ beta1 * x + beta2 * m m ~ alpha1 * x indirect := alpha1 * beta2 &quot; # With Mean Structure model &lt;- &quot; y ~ beta1 * x + beta2 * m m ~ alpha1 * x y ~~ sigma2ey * y m ~~ sigma2em * m x ~~ sigma2x * x y ~ beta0 * 1 m ~ alpha0 * 1 x ~ mux * 1 indirect := alpha1 * beta2 &quot; fit &lt;- lavaan::sem( model, data = df, meanstructure = TRUE, fixed.x = FALSE ) lavaan::summary(fit) ## lavaan 0.6-7 ended normally after 28 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 9 ## ## Number of observations 1000 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## y ~ ## x (bet1) 0.045 0.031 1.449 0.147 ## m (bet2) 0.471 0.031 15.034 0.000 ## m ~ ## x (alp1) 0.482 0.027 17.636 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .y (bet0) 48.479 3.191 15.193 0.000 ## .m (alp0) 52.045 2.771 18.784 0.000 ## x (mux) 100.320 0.479 209.300 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .y (sgm2y) 167.964 7.512 22.361 0.000 ## .m (sgm2m) 171.335 7.662 22.361 0.000 ## x (sgm2x) 229.740 10.274 22.361 0.000 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ## indirect 0.227 0.020 11.441 0.000 8.2.3 Using the ramR Package A ## y m x ey em ## y 0 0.4707226 0.04486201 1 0 ## m 0 0.0000000 0.48162865 0 1 ## x 0 0.0000000 0.00000000 0 0 ## ey 0 0.0000000 0.00000000 0 0 ## em 0 0.0000000 0.00000000 0 0 S ## y m x ey em ## y 0 0 0.0000 0.000 0.0000 ## m 0 0 0.0000 0.000 0.0000 ## x 0 0 229.9701 0.000 0.0000 ## ey 0 0 0.0000 168.469 0.0000 ## em 0 0 0.0000 0.000 171.6784 u ## u ## y 48.47917 ## m 52.04456 ## x 100.31982 ## ey 0.00000 ## em 0.00000 filter ## y m x ey em ## y 1 0 0 0 0 ## m 0 1 0 0 0 ## x 0 0 1 0 0 The covariance expectations can be numerically derived using the ramR::C() function. ramR::C(A, S) ## y m x ey em ## y 223.47047 110.8927 62.45425 168.469 80.81291 ## m 110.89267 225.0237 110.76020 0.000 171.67843 ## x 62.45425 110.7602 229.97013 0.000 0.00000 ## ey 168.46896 0.0000 0.00000 168.469 0.00000 ## em 80.81291 171.6784 0.00000 0.000 171.67843 The covariance expectations for the observed variables can be numerically derived using the ramR::M() function. ramR::M(A, S, filter) ## y m x ## y 223.47047 110.8927 62.45425 ## m 110.89267 225.0237 110.76020 ## x 62.45425 110.7602 229.97013 The mean expectations can be numerically derived using the ramR::v() function. ramR::v(A, u) ## v ## y 100.2221 ## m 100.3615 ## x 100.3198 ## ey 0.0000 ## em 0.0000 The mean expectations for the observed variables can be numerically derived using the ramR::v() function. ramR::g(A, u, filter) ## g ## y 100.2221 ## m 100.3615 ## x 100.3198 8.3 Equations to RAM The ramR package has a utility function to convert structural equations to RAM notation. The simple mediation model can be expressed in the following equations eq &lt;- &quot; # VARIABLE1 OPERATION VARIABLE2 LABEL ey by y 1 em by m 1 y on x beta[1] y on m beta[2] m on x alpha[1] ey with ey sigma[varepsilon[y]]^2 em with em sigma[varepsilon[m]]^2 x with x sigma[x]^2 y on 1 beta[0] m on 1 alpha[0] x on 1 mu[x] &quot; The ramR::Eq2RAM converts the equations to RAM notation. ramR::Eq2RAM(eq) ## $par.table ## lhs op rhs par.label par.index ## 1 ey by y 1 1 ## 2 em by m 1 1 ## 3 y on x beta[1] p1 ## 4 y on m beta[2] p2 ## 5 m on x alpha[1] p3 ## 6 ey with ey sigma[varepsilon[y]]^2 p4 ## 7 em with em sigma[varepsilon[m]]^2 p5 ## 8 x with x sigma[x]^2 p6 ## 9 y on 1 beta[0] p7 ## 10 m on 1 alpha[0] p8 ## 11 x on 1 mu[x] p9 ## ## $variables ## [1] &quot;y&quot; &quot;m&quot; &quot;x&quot; &quot;ey&quot; &quot;em&quot; ## ## $g.variables ## [1] &quot;y&quot; &quot;m&quot; &quot;x&quot; ## ## $h.variables ## [1] &quot;ey&quot; &quot;em&quot; ## ## $A ## y m x ey em ## y &quot;0&quot; &quot;beta[2]&quot; &quot;beta[1]&quot; &quot;1&quot; &quot;0&quot; ## m &quot;0&quot; &quot;0&quot; &quot;alpha[1]&quot; &quot;0&quot; &quot;1&quot; ## x &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## ey &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## em &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## ## $S ## y m x ey em ## y &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## m &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ## x &quot;0&quot; &quot;0&quot; &quot;sigma[x]^2&quot; &quot;0&quot; &quot;0&quot; ## ey &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;sigma[varepsilon[y]]^2&quot; &quot;0&quot; ## em &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;sigma[varepsilon[m]]^2&quot; ## ## $u ## u ## y &quot;beta[0]&quot; ## m &quot;alpha[0]&quot; ## x &quot;mu[x]&quot; ## ey &quot;0&quot; ## em &quot;0&quot; ## ## $Filter ## y m x ey em ## y 1 0 0 0 0 ## m 0 1 0 0 0 ## x 0 0 1 0 0 8.4 Equations to Expectations The ramR package has a utility function to convert structural equations to expectations both symbolically and numerically. eq &lt;- &quot; # VARIABLE1 OPERATION VARIABLE2 LABEL ey by y 1 em by m 1 y on x beta[1] y on m beta[2] m on x alpha[1] ey with ey sigma[varepsilon[y]]^2 em with em sigma[varepsilon[m]]^2 x with x sigma[x]^2 y on 1 beta[0] m on 1 alpha[0] x on 1 mu[x] &quot; ramR::Eq2Expectations(eq) ## $par.table ## lhs op rhs par.label par.index ## 1 ey by y 1 1 ## 2 em by m 1 1 ## 3 y on x beta[1] p1 ## 4 y on m beta[2] p2 ## 5 m on x alpha[1] p3 ## 6 ey with ey sigma[varepsilon[y]]^2 p4 ## 7 em with em sigma[varepsilon[m]]^2 p5 ## 8 x with x sigma[x]^2 p6 ## 9 y on 1 beta[0] p7 ## 10 m on 1 alpha[0] p8 ## 11 x on 1 mu[x] p9 ## ## $variables ## [1] &quot;y&quot; &quot;m&quot; &quot;x&quot; &quot;ey&quot; &quot;em&quot; ## ## $g.variables ## [1] &quot;y&quot; &quot;m&quot; &quot;x&quot; ## ## $h.variables ## [1] &quot;ey&quot; &quot;em&quot; ## ## $A ## {{ 0, beta[2], beta[1], 1, 0}, ## { 0, 0, alpha[1], 0, 1}, ## { 0, 0, 0, 0, 0}, ## { 0, 0, 0, 0, 0}, ## { 0, 0, 0, 0, 0}} ## ## $S ## {{ 0, 0, 0, 0, 0}, ## { 0, 0, 0, 0, 0}, ## { 0, 0, sigma[x]^2, 0, 0}, ## { 0, 0, 0, sigma[varepsilon[y]]^2, 0}, ## { 0, 0, 0, 0, sigma[varepsilon[m]]^2}} ## ## $u ## {{ beta[0]}, ## {alpha[0]}, ## { mu[x]}, ## { 0}, ## { 0}} ## ## $Filter ## {{1, 0, 0, 0, 0}, ## {0, 1, 0, 0, 0}, ## {0, 0, 1, 0, 0}} ## ## $v ## {{beta[0]+beta[2]*alpha[0]+(beta[1]+beta[2]*alpha[1])*mu[x]}, ## { alpha[0]+alpha[1]*mu[x]}, ## { mu[x]}, ## { 0}, ## { 0}} ## ## $g ## {{beta[0]+beta[2]*alpha[0]+(beta[1]+beta[2]*alpha[1])*mu[x]}, ## { alpha[0]+alpha[1]*mu[x]}, ## { mu[x]}} ## ## $C ## {{sigma[x]^2*(beta[1]+beta[2]*alpha[1])^2+sigma[varepsilon[y]]^2+sigma[varepsilon[m]]^2*beta[2]^2, (beta[1]+beta[2]*alpha[1])*sigma[x]^2*alpha[1]+beta[2]*sigma[varepsilon[m]]^2, (beta[1]+beta[2]*alpha[1])*sigma[x]^2, sigma[varepsilon[y]]^2, beta[2]*sigma[varepsilon[m]]^2}, ## { alpha[1]*sigma[x]^2*(beta[1]+beta[2]*alpha[1])+sigma[varepsilon[m]]^2*beta[2], sigma[x]^2*alpha[1]^2+sigma[varepsilon[m]]^2, alpha[1]*sigma[x]^2, 0, sigma[varepsilon[m]]^2}, ## { sigma[x]^2*(beta[1]+beta[2]*alpha[1]), sigma[x]^2*alpha[1], sigma[x]^2, 0, 0}, ## { sigma[varepsilon[y]]^2, 0, 0, sigma[varepsilon[y]]^2, 0}, ## { sigma[varepsilon[m]]^2*beta[2], sigma[varepsilon[m]]^2, 0, 0, sigma[varepsilon[m]]^2}} ## ## $M ## {{sigma[x]^2*(beta[1]+beta[2]*alpha[1])^2+sigma[varepsilon[y]]^2+sigma[varepsilon[m]]^2*beta[2]^2, (beta[1]+beta[2]*alpha[1])*sigma[x]^2*alpha[1]+beta[2]*sigma[varepsilon[m]]^2, (beta[1]+beta[2]*alpha[1])*sigma[x]^2}, ## { alpha[1]*sigma[x]^2*(beta[1]+beta[2]*alpha[1])+sigma[varepsilon[m]]^2*beta[2], sigma[x]^2*alpha[1]^2+sigma[varepsilon[m]]^2, alpha[1]*sigma[x]^2}, ## { sigma[x]^2*(beta[1]+beta[2]*alpha[1]), sigma[x]^2*alpha[1], sigma[x]^2}} eq &lt;- &quot; # VARIABLE1 OPERATION VARIABLE2 LABEL ey by y 1 em by m 1 y on x 0.00 y on m 0.50 m on x 0.50 ey with ey 168.75 em with em 168.75 x with x 225 y on 1 50 m on 1 50 x on 1 100 &quot; ramR::Eq2Expectations(eq) ## $par.table ## lhs op rhs par.label par.index ## 1 ey by y 1.00 1.00 ## 2 em by m 1.00 1.00 ## 3 y on x 0.00 0.00 ## 4 y on m 0.50 0.50 ## 5 m on x 0.50 0.50 ## 6 ey with ey 168.75 168.75 ## 7 em with em 168.75 168.75 ## 8 x with x 225.00 225.00 ## 9 y on 1 50.00 50.00 ## 10 m on 1 50.00 50.00 ## 11 x on 1 100.00 100.00 ## ## $variables ## [1] &quot;y&quot; &quot;m&quot; &quot;x&quot; &quot;ey&quot; &quot;em&quot; ## ## $g.variables ## [1] &quot;y&quot; &quot;m&quot; &quot;x&quot; ## ## $h.variables ## [1] &quot;ey&quot; &quot;em&quot; ## ## $A ## y m x ey em ## y 0 0.5 0.0 1 0 ## m 0 0.0 0.5 0 1 ## x 0 0.0 0.0 0 0 ## ey 0 0.0 0.0 0 0 ## em 0 0.0 0.0 0 0 ## ## $S ## y m x ey em ## y 0 0 0 0.00 0.00 ## m 0 0 0 0.00 0.00 ## x 0 0 225 0.00 0.00 ## ey 0 0 0 168.75 0.00 ## em 0 0 0 0.00 168.75 ## ## $u ## u ## y 50 ## m 50 ## x 100 ## ey 0 ## em 0 ## ## $Filter ## y m x ey em ## y 1 0 0 0 0 ## m 0 1 0 0 0 ## x 0 0 1 0 0 ## ## $v ## v ## y 100 ## m 100 ## x 100 ## ey 0 ## em 0 ## ## $g ## g ## y 100 ## m 100 ## x 100 ## ## $C ## y m x ey em ## y 225.000 112.50 56.25 168.75 84.375 ## m 112.500 225.00 112.50 0.00 168.750 ## x 56.250 112.50 225.00 0.00 0.000 ## ey 168.750 0.00 0.00 168.75 0.000 ## em 84.375 168.75 0.00 0.00 168.750 ## ## $M ## y m x ## y 225.00 112.5 56.25 ## m 112.50 225.0 112.50 ## x 56.25 112.5 225.00 "],["ram-med-simple-std.html", "Chapter 9 The Standardized Simple Mediation Model", " Chapter 9 The Standardized Simple Mediation Model Figure 9.1: The Standardized Simple Mediation Model "],["references.html", "References", " References Boker, Steven M., and John J. McArdle. 2005. “Path Analysis and Path Diagrams.” In Encyclopedia of Statistics in Behavioral Science, edited by Brian S. Everitt and David C. Howell, 1529–31. Chichester, UK: John Wiley &amp; Sons, Ltd. https://doi.org/10.1002/0470013192.bsa471. Boker, Steven M., Michael C. Neale, Hermine H. Maes, Michael J. Wilde, Michael Spiegel, Timothy R. Brick, Ryne Estabrook, et al. 2020. OpenMx 2.18.1 User Guide. McArdle, John J. 2005. “The Development of the RAM Rules for Latent Variable Structural Equation Modeling.” In Contemporary Psychometrics: A Festschrift for Roderick P. McDonald, edited by Albert Maydeu-Olivares and John J. McArdle, 225–73. Multivariate Applications Book Series. Mahwah, NJ: Lawrence Erlbaum Associates. McArdle, John J., and Roderick P. McDonald. 1984. “Some Algebraic Properties of the Reticular Action Model for Moment Structures.” British Journal of Mathematical and Statistical Psychology 37 (2): 234–51. https://doi.org/10.1111/j.2044-8317.1984.tb00802.x. Pesigan, Ivan Jacob Agaloos. 2021. ramR: Reticular Action Model (RAM) Notation. https://github.com/jeksterslab/ramR. R Core Team. 2020. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/. Rosseel, Yves. 2012. “lavaan: An R Package for Structural Equation Modeling.” Journal of Statistical Software 48 (2): 1–36. http://www.jstatsoft.org/v48/i02/. "]]
